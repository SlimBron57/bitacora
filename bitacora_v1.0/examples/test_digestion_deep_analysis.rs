//! # ğŸ”¬ Deep Analysis of Digested Data
//!
//! **PropÃ³sito:** Analizar profundamente la informaciÃ³n ingerida para evaluar:
//! 1. Calidad estructural del parsing
//! 2. Riqueza semÃ¡ntica capturada
//! 3. Gaps en informaciÃ³n
//! 4. PreparaciÃ³n para Phases 7.x.3-7.x.5 (Extraction â†’ Distribution)
//!
//! **Ejecutar:**
//! ```bash
//! cargo run --example test_digestion_deep_analysis
//! ```

use bitacora::data_import::digestion::{ContentType, DigestionPipeline, WhatsAppDigester};
use bitacora::data_import::{DataSource, QuarantineZone};
use chrono::{DateTime, Datelike, Timelike, Utc, Weekday};
use std::collections::HashMap;

#[tokio::main]
async fn main() {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  ğŸ”¬ DEEP ANALYSIS OF DIGESTED DATA - Paula Roque Chat      â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ==============================================================
    // ğŸ”¹ PHASE 1: Load & Digest
    // ==============================================================
    let file_path = "/home/edgi/Documents/Development/own/bitacora/bitacora_v1.0/data/imports/whatsapp/_chat.txt";
    
    print!("ğŸ“‚ Loading file... ");
    let raw_data = match std::fs::read(file_path) {
        Ok(data) => {
            println!("âœ“ ({:.2} KB)", data.len() as f64 / 1024.0);
            data
        }
        Err(e) => {
            eprintln!("âŒ Error: {}", e);
            return;
        }
    };

    print!("ğŸ“‹ Quarantine inspection... ");
    let mut quarantine = match QuarantineZone::inspect(raw_data, DataSource::WhatsApp).await {
        Ok(q) => {
            println!("âœ“");
            q
        }
        Err(e) => {
            eprintln!("âŒ Error: {}", e);
            return;
        }
    };
    quarantine.approve();

    print!("ğŸ”¬ Digesting data... ");
    let digester = WhatsAppDigester::new();
    let digested = match digester.digest(&quarantine).await {
        Ok(d) => {
            println!("âœ“ ({} entries, {}ms)", d.entries.len(), d.digestion_metadata.duration_ms);
            d
        }
        Err(e) => {
            eprintln!("âŒ Error: {}", e);
            return;
        }
    };

    println!("\n{}", "â”€".repeat(66));
    
    // ==============================================================
    // ğŸ”¹ PHASE 2: Structural Quality Analysis
    // ==============================================================
    println!("\nğŸ“Š SECTION 1: STRUCTURAL QUALITY");
    println!("{}", "â•".repeat(66));

    // 2.1 Parsing completeness
    let success_rate = digested.entries.len() as f64 
        / (digested.entries.len() + digested.digestion_metadata.failed_count) as f64 * 100.0;
    
    println!("\nâœ… Parsing Completeness:");
    println!("   â€¢ Success rate: {:.2}%", success_rate);
    println!("   â€¢ Total entries: {}", digested.entries.len());
    println!("   â€¢ Failed entries: {}", digested.digestion_metadata.failed_count);
    println!("   â€¢ Performance: {}ms ({:.2} msgs/ms)", 
        digested.digestion_metadata.duration_ms,
        digested.entries.len() as f64 / digested.digestion_metadata.duration_ms as f64
    );

    // 2.2 Data integrity
    let mut entries_with_author = 0;
    let mut entries_without_author = 0;
    let mut entries_with_content = 0;
    let mut empty_content = 0;

    for entry in &digested.entries {
        if entry.author.is_some() {
            entries_with_author += 1;
        } else {
            entries_without_author += 1;
        }
        
        if !entry.content.is_empty() {
            entries_with_content += 1;
        } else {
            empty_content += 1;
        }
    }

    println!("\nğŸ” Data Integrity:");
    println!("   â€¢ Entries with author: {} ({:.1}%)", 
        entries_with_author,
        entries_with_author as f64 / digested.entries.len() as f64 * 100.0
    );
    println!("   â€¢ Entries without author (system): {} ({:.1}%)", 
        entries_without_author,
        entries_without_author as f64 / digested.entries.len() as f64 * 100.0
    );
    println!("   â€¢ Entries with content: {} ({:.1}%)", 
        entries_with_content,
        entries_with_content as f64 / digested.entries.len() as f64 * 100.0
    );
    
    if empty_content > 0 {
        println!("   âš ï¸  Empty content entries: {}", empty_content);
    }

    // 2.3 Content type distribution
    let mut text_count = 0;
    let mut photo_count = 0;
    let mut audio_count = 0;
    let mut video_count = 0;
    let mut sticker_count = 0;
    let mut attachment_count = 0;
    let mut system_count = 0;

    for entry in &digested.entries {
        match &entry.content_type {
            ContentType::Text => text_count += 1,
            ContentType::Multimedia { mime_type } => {
                if mime_type.contains("image") {
                    photo_count += 1;
                } else if mime_type.contains("audio") {
                    audio_count += 1;
                } else if mime_type.contains("video") {
                    video_count += 1;
                } else if mime_type.contains("sticker") {
                    sticker_count += 1;
                }
            }
            ContentType::Attachment { .. } => attachment_count += 1,
            ContentType::System => system_count += 1,
        }
    }

    println!("\nğŸ“¦ Content Type Distribution:");
    println!("   â€¢ Text messages: {} ({:.1}%)", 
        text_count, text_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ Photos: {} ({:.1}%)", 
        photo_count, photo_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ Audio: {} ({:.1}%)", 
        audio_count, audio_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ Video: {} ({:.1}%)", 
        video_count, video_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ Stickers: {} ({:.1}%)", 
        sticker_count, sticker_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ Attachments: {} ({:.1}%)", 
        attachment_count, attachment_count as f64 / digested.entries.len() as f64 * 100.0);
    println!("   â€¢ System messages: {} ({:.1}%)", 
        system_count, system_count as f64 / digested.entries.len() as f64 * 100.0);

    // ==============================================================
    // ğŸ”¹ PHASE 3: Semantic Richness Analysis
    // ==============================================================
    println!("\n\nğŸ“Š SECTION 2: SEMANTIC RICHNESS");
    println!("{}", "â•".repeat(66));

    // 3.1 Author analysis
    let mut author_stats: HashMap<String, AuthorStats> = HashMap::new();
    
    for entry in &digested.entries {
        if let Some(author) = &entry.author {
            let stats = author_stats.entry(author.clone()).or_insert(AuthorStats::default());
            stats.total_messages += 1;
            stats.total_chars += entry.content.len();
            
            match &entry.content_type {
                ContentType::Text => stats.text_messages += 1,
                ContentType::Multimedia { .. } => stats.multimedia_messages += 1,
                _ => {}
            }
        }
    }

    println!("\nğŸ‘¥ Author Analysis:");
    for (author, stats) in author_stats.iter() {
        println!("\n   {} ({}% of conversation)", 
            author, 
            stats.total_messages as f64 / entries_with_author as f64 * 100.0
        );
        println!("      â€¢ Messages: {}", stats.total_messages);
        println!("      â€¢ Text: {} ({:.1}%)", 
            stats.text_messages, 
            stats.text_messages as f64 / stats.total_messages as f64 * 100.0
        );
        println!("      â€¢ Multimedia: {} ({:.1}%)", 
            stats.multimedia_messages,
            stats.multimedia_messages as f64 / stats.total_messages as f64 * 100.0
        );
        println!("      â€¢ Avg chars/msg: {:.1}", 
            stats.total_chars as f64 / stats.total_messages as f64
        );
    }

    // 3.2 Temporal patterns
    let first_entry = digested.entries.first().unwrap();
    let last_entry = digested.entries.last().unwrap();
    let duration_days = (last_entry.timestamp - first_entry.timestamp).num_days();

    println!("\n\nğŸ“… Temporal Patterns:");
    println!("   â€¢ Date range: {} â†’ {}", 
        first_entry.timestamp.format("%Y-%m-%d"),
        last_entry.timestamp.format("%Y-%m-%d")
    );
    println!("   â€¢ Duration: {} days ({:.1} months)", 
        duration_days, 
        duration_days as f64 / 30.0
    );
    println!("   â€¢ Avg messages/day: {:.1}", 
        digested.entries.len() as f64 / duration_days as f64
    );

    // 3.3 Hourly distribution
    let mut hourly_distribution: HashMap<u32, usize> = HashMap::new();
    for entry in &digested.entries {
        *hourly_distribution.entry(entry.timestamp.hour()).or_insert(0) += 1;
    }

    println!("\n   â° Activity by hour (top 5):");
    let mut hourly_vec: Vec<_> = hourly_distribution.iter().collect();
    hourly_vec.sort_by(|a, b| b.1.cmp(a.1));
    for (hour, count) in hourly_vec.iter().take(5) {
        println!("      â€¢ {:02}:00-{:02}:59 â†’ {} messages ({:.1}%)",
            hour, hour, count,
            **count as f64 / digested.entries.len() as f64 * 100.0
        );
    }

    // 3.4 Weekly distribution
    let mut weekly_distribution: HashMap<Weekday, usize> = HashMap::new();
    for entry in &digested.entries {
        *weekly_distribution.entry(entry.timestamp.weekday()).or_insert(0) += 1;
    }

    println!("\n   ğŸ“† Activity by weekday:");
    let weekdays = [
        Weekday::Mon, Weekday::Tue, Weekday::Wed, 
        Weekday::Thu, Weekday::Fri, Weekday::Sat, Weekday::Sun
    ];
    for day in weekdays.iter() {
        let count = weekly_distribution.get(day).unwrap_or(&0);
        println!("      â€¢ {:?} â†’ {} messages ({:.1}%)",
            day, count,
            *count as f64 / digested.entries.len() as f64 * 100.0
        );
    }

    // ==============================================================
    // ğŸ”¹ PHASE 4: Extraction Readiness (Phase 7.x.3)
    // ==============================================================
    println!("\n\nğŸ“Š SECTION 3: EXTRACTION READINESS (Phase 7.x.3)");
    println!("{}", "â•".repeat(66));

    println!("\nğŸ§¬ 7D Nutrient Extraction Analysis:");
    
    // 4.1 Biographical (Identity)
    let biographical_signals = analyze_biographical(&digested.entries);
    print_dimension_readiness("1. Biographical (Identity)", &biographical_signals);

    // 4.2 Interests (Topics)
    let interest_signals = analyze_interests(&digested.entries);
    print_dimension_readiness("2. Interests (Topics)", &interest_signals);

    // 4.3 Emotional (Tone)
    let emotional_signals = analyze_emotional(&digested.entries);
    print_dimension_readiness("3. Emotional (Tone)", &emotional_signals);

    // 4.4 Relationships (Social)
    let relationship_signals = analyze_relationships(&digested.entries);
    print_dimension_readiness("4. Relationships (Social)", &relationship_signals);

    // 4.5 Temporal (Patterns)
    let temporal_signals = analyze_temporal(&digested.entries);
    print_dimension_readiness("5. Temporal (Patterns)", &temporal_signals);

    // 4.6 Behavioral (Habits)
    let behavioral_signals = analyze_behavioral(&digested.entries);
    print_dimension_readiness("6. Behavioral (Habits)", &behavioral_signals);

    // 4.7 Hyperlinks (Consumption)
    let hyperlink_signals = analyze_hyperlinks(&digested.entries);
    print_dimension_readiness("7. Hyperlinks (Consumption)", &hyperlink_signals);

    // ==============================================================
    // ğŸ”¹ PHASE 5: Gaps & Recommendations
    // ==============================================================
    println!("\n\nğŸ“Š SECTION 4: GAPS & RECOMMENDATIONS");
    println!("{}", "â•".repeat(66));

    println!("\nâš ï¸  Current Limitations:");
    println!("   1. âŒ Multimedia content NOT analyzed (photos, audio, video)");
    println!("      â†’ Need: Sensory Engine (Phase 8.x)");
    println!("      â†’ Files available: {} photos, {} audio, {} video", 
        photo_count, audio_count, video_count);
    
    println!("\n   2. âŒ Text content NOT semantically extracted");
    println!("      â†’ Need: NutrientExtractor (Phase 7.x.3)");
    println!("      â†’ Text messages available: {}", text_count);
    
    println!("\n   3. âŒ No CTX7D tokenization");
    println!("      â†’ Need: Context Token System");
    println!("      â†’ Ready for: 7D dimensional analysis");
    
    println!("\n   4. âš ï¸  No relationship graph");
    println!("      â†’ Need: SocialGraph subsystem");
    println!("      â†’ Authors detected: {}", author_stats.len());

    println!("\n\nâœ… What We Have (Ready to Use):");
    println!("   âœ“ Perfect parsing (100% success rate)");
    println!("   âœ“ Temporal data (timestamps, duration, patterns)");
    println!("   âœ“ Author identification ({} participants)", author_stats.len());
    println!("   âœ“ Content type classification");
    println!("   âœ“ Message sequencing (chronological order)");
    println!("   âœ“ Multimedia detection (file references)");

    println!("\n\nğŸ¯ NEXT STEPS - Phase 7.x.3 Implementation:");
    println!("   1. Implement NutrientExtractor trait");
    println!("   2. BiographicalExtractor â†’ Extract identity signals");
    println!("   3. InterestExtractor â†’ Detect topics from text");
    println!("   4. EmotionalExtractor â†’ Analyze tone/sentiment");
    println!("   5. RelationshipExtractor â†’ Build social graph");
    println!("   6. TemporalExtractor â†’ Pattern detection");
    println!("   7. BehavioralExtractor â†’ Habit analysis");
    println!("   8. HyperlinkExtractor â†’ Consumption patterns");

    println!("\n{}", "â•".repeat(66));
    println!("ğŸ‰ Analysis complete. System ready for Phase 7.x.3 (Extraction)");
    println!("{}", "â•".repeat(66));
}

// ==============================================================
// Helper Structures
// ==============================================================

#[derive(Default)]
struct AuthorStats {
    total_messages: usize,
    text_messages: usize,
    multimedia_messages: usize,
    total_chars: usize,
}

struct DimensionSignals {
    available: Vec<String>,
    missing: Vec<String>,
    readiness: f64,
}

// ==============================================================
// Analysis Functions (Phase 7.x.3 Readiness)
// ==============================================================

fn analyze_biographical(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Check what signals are available
    if entries.iter().any(|e| e.author.is_some()) {
        available.push("Author names".to_string());
    }
    
    if entries.len() > 10 {
        available.push("Message history".to_string());
    }

    // What's missing
    missing.push("Name extraction from text".to_string());
    missing.push("Location detection".to_string());
    missing.push("Age/demographic hints".to_string());
    missing.push("Professional context".to_string());

    let readiness = available.len() as f64 / (available.len() + missing.len()) as f64;

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_interests(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Check text content
    let text_entries: Vec<_> = entries.iter()
        .filter(|e| matches!(e.content_type, ContentType::Text))
        .collect();
    
    if !text_entries.is_empty() {
        available.push(format!("{} text messages for topic extraction", text_entries.len()));
    }

    // What's missing
    missing.push("Topic detection (NLP)".to_string());
    missing.push("Keyword extraction".to_string());
    missing.push("Interest clustering".to_string());
    missing.push("Topic graph construction".to_string());

    let readiness = if !text_entries.is_empty() { 0.2 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_emotional(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Check multimedia (emotional signals)
    let multimedia_count = entries.iter()
        .filter(|e| matches!(e.content_type, ContentType::Multimedia { .. }))
        .count();
    
    if multimedia_count > 0 {
        available.push(format!("{} multimedia items (emotional context)", multimedia_count));
    }

    // Text for sentiment
    let text_count = entries.iter()
        .filter(|e| matches!(e.content_type, ContentType::Text))
        .count();
    
    if text_count > 0 {
        available.push(format!("{} text messages (sentiment analysis)", text_count));
    }

    // What's missing
    missing.push("Sentiment analysis".to_string());
    missing.push("Emotion classification".to_string());
    missing.push("Tone progression tracking".to_string());
    missing.push("EmotionalSpace mapping".to_string());

    let readiness = if !available.is_empty() { 0.15 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_relationships(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Count unique authors
    let mut authors: std::collections::HashSet<String> = std::collections::HashSet::new();
    for entry in entries {
        if let Some(author) = &entry.author {
            authors.insert(author.clone());
        }
    }

    if authors.len() >= 2 {
        available.push(format!("{} participants detected", authors.len()));
        available.push("Conversation structure".to_string());
    }

    // What's missing
    missing.push("Relationship strength analysis".to_string());
    missing.push("Interaction patterns".to_string());
    missing.push("Social graph construction".to_string());
    missing.push("Response time analysis".to_string());

    let readiness = if authors.len() >= 2 { 0.25 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_temporal(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // We have timestamps
    if !entries.is_empty() {
        available.push("Timestamps (all messages)".to_string());
        available.push("Chronological sequence".to_string());
        
        if let (Some(first), Some(last)) = (entries.first(), entries.last()) {
            let duration_days = (last.timestamp - first.timestamp).num_days();
            available.push(format!("{} days history", duration_days));
        }
    }

    // What's missing
    missing.push("Pattern detection (daily/weekly)".to_string());
    missing.push("Activity clustering".to_string());
    missing.push("Time series analysis".to_string());

    let readiness = if !entries.is_empty() { 0.6 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_behavioral(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Content types give behavioral hints
    let content_types: std::collections::HashSet<_> = entries.iter()
        .map(|e| format!("{:?}", e.content_type))
        .collect();
    
    if content_types.len() > 1 {
        available.push(format!("{} communication modes detected", content_types.len()));
    }

    // What's missing
    missing.push("Habit pattern detection".to_string());
    missing.push("Communication style analysis".to_string());
    missing.push("Behavioral profiling".to_string());
    missing.push("Usage pattern clustering".to_string());

    let readiness = if content_types.len() > 1 { 0.2 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn analyze_hyperlinks(entries: &[bitacora::data_import::digestion::DigestedEntry]) -> DimensionSignals {
    let mut available = Vec::new();
    let mut missing = Vec::new();

    // Check for URLs in text
    let mut url_count = 0;
    for entry in entries {
        if matches!(entry.content_type, ContentType::Text) {
            if entry.content.contains("http://") || entry.content.contains("https://") {
                url_count += 1;
            }
        }
    }

    if url_count > 0 {
        available.push(format!("{} messages with URLs", url_count));
    }

    // What's missing
    missing.push("URL extraction".to_string());
    missing.push("Content type classification".to_string());
    missing.push("Consumption pattern analysis".to_string());
    missing.push("Link graph construction".to_string());

    let readiness = if url_count > 0 { 0.15 } else { 0.0 };

    DimensionSignals {
        available,
        missing,
        readiness,
    }
}

fn print_dimension_readiness(name: &str, signals: &DimensionSignals) {
    let status = if signals.readiness >= 0.5 {
        "ğŸŸ¢ READY"
    } else if signals.readiness >= 0.2 {
        "ğŸŸ¡ PARTIAL"
    } else {
        "ğŸ”´ NOT READY"
    };

    println!("\n   {} - {} ({:.0}%)", status, name, signals.readiness * 100.0);
    
    if !signals.available.is_empty() {
        println!("      âœ“ Available:");
        for signal in &signals.available {
            println!("        - {}", signal);
        }
    }
    
    if !signals.missing.is_empty() {
        println!("      âœ— Missing:");
        for signal in &signals.missing {
            println!("        - {}", signal);
        }
    }
}
