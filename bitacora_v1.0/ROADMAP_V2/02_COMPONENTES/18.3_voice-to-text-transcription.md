```yaml
# === DATOS DE AUDITOR√çA ===
Archivo: ROADMAP_V2/02_COMPONENTES/18.3_voice-to-text-transcription.md
Versi√≥n: 1.0
Fecha Creaci√≥n: 2025-11-29
√öltima Actualizaci√≥n: 2025-11-29 04:00:00
Autor: Sistema Bit√°cora (B) + Eduardo
Prop√≥sito: Voice-to-Text transcription para WhatsApp audio imports (v1.0) + Bit√°cora Conversacional (v2.0)
Estado: ACTIVO - Strategic foundation para conversational interface
Parent Doc: 18_hyperlink-extractor.md (HyperlinkExtractor)
Inspiraci√≥n: OpenAI Whisper, AssemblyAI, Google Speech-to-Text, Deepgram
Filosof√≠a: "Audio imports v1.0 (almost free) ‚Üí Real-time conversation v2.0 (strategic investment)"
Cambios: Initial creation - v1.0 batch processing + v2.0 conversational vision
Punto de Entrada: Secci√≥n 1 (Preludio) ‚Üí Secci√≥n 3 (Filosof√≠a Dual) ‚Üí Secci√≥n 6 (Plan v1.0 + v2.0)
Related: 18.1_hyperlink-content-extraction.md (multimedia), 18.2_image-recognition-analysis.md (vision)
# === FIN DATOS DE AUDITOR√çA ===
```

# üéôÔ∏è Voice-to-Text Transcription

> **"La voz es el canal m√°s natural de comunicaci√≥n humana. Bit√°cora debe ESCUCHAR."**

---

## üìë √çNDICE

1. [Preludio: Los 86 Audios](#1-preludio-los-86-audios)
2. [Inventario F√≠sico](#2-inventario-f√≠sico)
3. [Filosof√≠a Dual: v1.0 vs v2.0](#3-filosof√≠a-dual-v10-vs-v20)
4. [Prop√≥sito del M√≥dulo](#4-prop√≥sito-del-m√≥dulo)
5. [Flujo L√≥gico](#5-flujo-l√≥gico)
6. [Plan de Acci√≥n Dual](#6-plan-de-acci√≥n-dual)
7. [Validaci√≥n](#7-validaci√≥n)
8. [Implementaci√≥n Rust](#8-implementaci√≥n-rust)
9. [Est√°ndares](#9-est√°ndares)
10. [Checklist](#10-checklist)

---

## 1. PRELUDIO: LOS 86 AUDIOS

### La Historia de Paula y Eduardo

Paula Roque chat: **1,354 mensajes, 86 audios compartidos** (6% del chat).

Eduardo escucha esos 86 audios:
- Nota de voz contando an√©cdota graciosa (3 min)
- Mensaje urgente "Ll√°mame cuando puedas" (10 seg)
- Reflexi√≥n profunda sobre la vida (8 min)
- Risa espont√°nea sin palabras (5 seg)
- Explicaci√≥n compleja de proyecto laboral (12 min)
- Canci√≥n cantada improvisadamente (1 min)
- "Te quiero mucho" con tono emocional (8 seg)

**Duraci√≥n promedio:** ~2 minutos por audio  
**Duraci√≥n total:** ~172 minutos = 2.8 horas de contenido

### Las Dos Caras del Audio

```yaml
Face 1 - WhatsApp Imports (v1.0):
  Context: Historical messages
  Timing: Batch processing (no rush)
  Volume: 86 audios (one-time per chat)
  Budget: Limited ($5-10/chat acceptable)
  Priority: Content extraction (qu√© dijeron)
  
Face 2 - Bit√°cora Conversacional (v2.0):
  Context: Real-time conversation
  Timing: <2s latency CRITICAL
  Volume: Continuous (daily conversations)
  Budget: Ongoing ($X/user/month)
  Priority: Natural interaction (flow, not friction)
```

**Key Insight:** v1.0 y v2.0 necesitan DIFERENTES estrategias.

### El Dilema: Batch vs Real-Time

```yaml
Approach 1 - OpenAI Whisper API (Cloud):
  Pros:
    - State-of-the-art accuracy (~95%+)
    - Multi-language (99 languages)
    - Automatic punctuation, capitalization
    - Speaker diarization (v3)
    - Timestamps
  
  Contras:
    - Cost: $0.006/minute ($0.36/hour)
      * 172 min x $0.006 = $1.03 per chat (v1.0 OK)
      * 30 min/day x $0.006 x 30 days = $5.40/user/month (v2.0 escalates)
    - Latency: ~3-5 segundos (v1.0 OK, v2.0 friction)
    - Privacy: audio va a OpenAI

Approach 2 - Whisper Local (whisper.cpp):
  Pros:
    - FREE (zero API costs)
    - Privacy (offline processing)
    - Low latency (<1s con GPU, ~2-4s CPU)
    - Control total
  
  Contras:
    - Requiere modelo local (1.5GB-3GB disk)
    - CPU/GPU requirements:
      * tiny model: CPU OK, accuracy ~80%
      * base model: CPU OK, accuracy ~85%
      * small model: CPU slow, accuracy ~90%
      * medium model: GPU recomendado, accuracy ~93%
      * large model: GPU necesario, accuracy ~95%
    - Mantenimiento: model updates
    - Implementaci√≥n compleja (FFI, C++ bindings)

Approach 3 - Hybrid (v1.0 API, v2.0 Local):
  Strategy:
    - v1.0 imports: Whisper API (batch, $1-2/chat acceptable)
    - v2.0 real-time: whisper.cpp local (free, <1s)
  
  Pros:
    - Best of both worlds
    - v1.0: Alta accuracy sin complejidad
    - v2.0: Free + fast para conversaciones
  
  Contras:
    - Doble implementaci√≥n
    - v2.0: Requiere local setup (GPU recomendado)

Decisi√≥n: ‚úÖ Hybrid Approach
  v1.0 (WhatsApp imports): OpenAI Whisper API
    - $0.006/min x 172 min = $1.03/chat (acceptable)
    - Alta accuracy (95%+)
    - Simple implementation
    - NO bloquea MVP
  
  v2.0 (Conversational): whisper.cpp local
    - FREE ongoing (after setup)
    - <1s latency con GPU
    - Privacy-first
    - Implemented cuando tengamos usuarios activos
```

---

## 2. INVENTARIO F√çSICO

### Archivos Relacionados

```bash
# Existing references
ROADMAP_V2/CHECKLIST_V2.md              # "86 audio" in Paula chat, Phase 9.3 Whisper API
ROADMAP_V2/02_COMPONENTES/01_sensory-engine.md  # AudioTranscriber STUB
ROADMAP_V2/02_COMPONENTES/18.1_hyperlink-content-extraction.md  # Audio extraction

# Files to create
src/data_import/content_extractors/audio.rs          # AudioTranscriber v1.0 (Whisper API)
src/sensory_engine/audio_transcriber.rs              # v2.0 (whisper.cpp local)
examples/test_audio_transcriber.rs                   # Testing
```

### Dependencias v1.0 (Whisper API)

```toml
[dependencies]
# Whisper API integration
reqwest = { version = "0.11", features = ["multipart"] }  # File upload
tokio = { version = "1", features = ["fs"] }  # Async file I/O
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Audio preprocessing (optional, for format conversion)
# ffmpeg-next = "6.0"  # If we need to convert to MP3/FLAC
```

### Dependencias v2.0 (whisper.cpp Local)

```toml
[dependencies]
# Whisper local (whisper.cpp bindings)
whisper-rs = "0.10"  # Rust bindings for whisper.cpp
# OR
# whispercpp = "0.1"  # Alternative binding

# Audio processing
hound = "3.5"        # WAV reading
symphonia = "0.5"    # Multi-format audio decoding
```

---

## 3. FILOSOF√çA DUAL: V1.0 VS V2.0

### Principios de Dise√±o

**1. v1.0: Content Extraction First (Whisper API)**

```yaml
Context: WhatsApp imports (historical data)
Philosophy: "Accuracy > Cost, Content > Speed"

Why Whisper API:
  - Alta accuracy (95%+) crucial para content extraction
  - Batch processing (no latency concerns)
  - Simple implementation (HTTP API)
  - Cost acceptable ($1-2/chat one-time)
  - Multi-language autom√°tico (detect idioma)
  - Punctuation + capitalization autom√°tico
  - NO bloquea MVP (fast implementation)

Trade-offs Accepted:
  - Cost: $0.006/min (mitigable con cache)
  - Privacy: audio va a OpenAI (v1.0 imports only)
  - Latency: ~3-5s (OK para batch)
```

**2. v2.0: Real-Time Conversation (whisper.cpp Local)**

```yaml
Context: Bit√°cora Conversacional (real-time interaction)
Philosophy: "Natural Flow > Perfection, Privacy > Cloud"

Why whisper.cpp Local:
  - FREE ongoing (no per-minute costs)
  - <1s latency con GPU (natural conversation flow)
  - Privacy-first (offline processing)
  - Scales con usage (no API limits)
  - Control total (model selection, optimization)

Requirements:
  - GPU recomendado (NVIDIA/AMD, CUDA/ROCm)
  - 1.5-3GB disk (model weights)
  - Rust FFI bindings (whisper-rs)
  - Model management (download, cache)

Trade-offs Accepted:
  - Complexity: C++ bindings, GPU setup
  - Setup: usuarios necesitan GPU (o CPU con patience)
  - Accuracy: ~90-93% (vs 95% API, acceptable para conversaci√≥n)
```

**3. Hybrid Strategy**

```
v1.0 (Now) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> v2.0 (Future)
Whisper API                   whisper.cpp Local
(Batch imports)               (Real-time conversation)
    ‚Üì                              ‚Üì
$1-2/chat one-time           $0/month ongoing
High accuracy                 Fast latency
Simple implementation         Complex setup
NO MVP blocker                Strategic investment
```

---

## 4. PROP√ìSITO DEL M√ìDULO

### Objetivo Dual

**v1.0: WhatsApp Audio Import Transcription**
- Transcribir 86 audios de Paula Roque chat
- Extraer contenido (qu√© dijeron)
- Detectar idioma autom√°ticamente
- Identificar emociones en voz (tono, pitch)
- Budget tracking ($5/day limit)

**v2.0: Bit√°cora Conversacional Real-Time**
- Transcripci√≥n en <1s (natural flow)
- Conversaci√≥n bidireccional (user habla ‚Üí Bit√°cora responde)
- Privacy-first (local processing)
- FREE ongoing (despu√©s de setup)
- Foundation para voice interface

### Audiencia

- v1.0: Eduardo + Developers (WhatsApp imports)
- v2.0: End users (conversational interface)

### Resultado Esperado

**v1.0 Deliverables:**
- ‚úÖ 86 audios transcritos (Paula chat)
- ‚úÖ Transcripts almacenados como nutrients
- ‚úÖ Budget tracking ($5/day, alert at 80%)
- ‚úÖ Language detection
- ‚úÖ Cache layer (30 d√≠as, audio doesn't change)

**v2.0 Vision:**
- ‚úÖ Real-time transcription (<1s latency)
- ‚úÖ Voice interface para Bit√°cora
- ‚úÖ Natural conversation flow
- ‚úÖ Offline/privacy-first

---

## 5. FLUJO L√ìGICO

### 5.1. v1.0 Flow (Whisper API)

```yaml
Phase 1 - Detect Audio Attachments:
  Input: DigestedData (WhatsApp import)
  Process:
    - Scan for <attached: filename.opus> markers
    - Classify by MIME type: audio/ogg, audio/mpeg, audio/wav
    - Extract file path from import directory
  Output: List<AudioPath>

Phase 2 - Preprocess Audio:
  Input: AudioPath
  Process:
    - Check format: OPUS/OGG/MP3/WAV/FLAC
    - Convert if needed (Whisper supports all, but MP3/FLAC preferred)
    - Optional: Extract duration (ffprobe)
    - Check file size (< 25MB Whisper limit)
  Output: AudioFile

Phase 3 - Check Cache:
  - Generate cache key: SHA256(audio_path + file_hash)
  - Check if transcription exists (30 d√≠as cache)
  - If HIT: Return cached transcript
  - If MISS: Proceed to API call

Phase 4 - Budget Check:
  - Check daily spending ($5/day limit)
  - Estimate cost: duration_minutes x $0.006
  - If would exceed: Skip or alert
  - If OK: Proceed

Phase 5 - Whisper API Call:
  Endpoint: POST https://api.openai.com/v1/audio/transcriptions
  Headers:
    Authorization: Bearer {API_KEY}
    Content-Type: multipart/form-data
  
  Body:
    file: @audio_file
    model: "whisper-1"
    language: "es"  # Optional, auto-detect if omitted
    response_format: "verbose_json"  # Includes timestamps
    temperature: 0.0  # Deterministic
  
  Response:
    {
      "text": "Hola, c√≥mo est√°s...",
      "language": "es",
      "duration": 120.5,
      "segments": [
        {
          "text": "Hola, c√≥mo est√°s",
          "start": 0.0,
          "end": 2.5
        },
        ...
      ]
    }

Phase 6 - Post-Processing:
  - Parse JSON response
  - Extract full transcript
  - Extract segments (timestamps)
  - Language detection
  - Calculate cost: duration x $0.006
  - Record in budget tracker
  - Metadata:
    * transcribed_at: timestamp
    * model: "whisper-1"
    * language: detected language
    * duration: seconds
    * cost: USD

Phase 7 - Cache Result:
  - Store transcript + metadata
  - TTL: 30 d√≠as (audio doesn't change)
  - Key: SHA256(path + hash)

Phase 8 - Convert to Nutrients:
  - Dimension: "AUDIO_CONTENT"
  - Key: audio_transcript_{filename}
  - Value: full transcript
  - Context: duration, language, timestamp
  - Confidence: 0.95 (Whisper accuracy)
```

### 5.2. v2.0 Flow (whisper.cpp Local)

```yaml
Phase 1 - Setup (One-Time):
  1. Download whisper model:
     - tiny.bin (75MB, fast, ~80% accuracy)
     - base.bin (142MB, balanced, ~85%)
     - small.bin (466MB, good, ~90%)
     - medium.bin (1.5GB, great, ~93%)
  
  2. Detect hardware:
     - Check GPU availability (CUDA/ROCm)
     - Fallback to CPU if no GPU
     - Recommend GPU for <1s latency
  
  3. Initialize whisper context:
     - Load model into memory
     - Configure inference parameters
     - Warm up (first inference)

Phase 2 - Real-Time Transcription:
  Input: Audio stream (microphone)
  
  1. Audio Capture:
     - Capture audio chunks (100ms buffers)
     - Sample rate: 16kHz (Whisper requirement)
     - Format: f32 samples
  
  2. Voice Activity Detection (VAD):
     - Detect speech vs silence
     - Buffer speech chunks
     - Trigger transcription on pause (>300ms silence)
  
  3. Transcription:
     - Feed audio buffer to whisper.cpp
     - Inference: <1s with GPU, ~2-4s with CPU
     - Output: text segment
  
  4. Streaming Output:
     - Return text immediately
     - No waiting for full sentence
     - Natural conversation flow

Phase 3 - Integration con Bit√°cora:
  1. User speaks ‚Üí transcribed
  2. Text sent to ShuiDao Router
  3. Bit√°cora processes + generates response
  4. Response converted to speech (TTS)
  5. User hears Bit√°cora's voice
```

---

## 6. PLAN DE ACCI√ìN DUAL

### 6.1. Roadmap v1.0 (Whisper API)

```yaml
Phase 1 (v1.0) - Whisper API Integration:
  Scope: Batch transcription para WhatsApp imports
  Tasks:
    1. Create AudioTranscriber struct (Whisper API client)
    2. Implement multipart file upload (reqwest)
    3. Implement API call (POST /audio/transcriptions)
    4. JSON parsing + validation
    5. Cache layer (30 d√≠as)
    6. Budget tracker ($5/day limit)
    7. Integration con HyperlinkExtractor
    8. Error handling (API failures, timeouts)
    9. Language detection
    10. Cost tracking
  
  Testing:
    - Test con 10 audios de Paula chat
    - Validate transcription accuracy (manual check)
    - Check language detection
    - Benchmark cost ($0.006/min expected)
    - Test cache hits
    - Test budget limit
  
  Effort: 3-4 d√≠as
  Priority: MEDIUM (v1.0 feature, enhances content extraction)
  Blockers: None (Whisper API available, simple HTTP)

Success Criteria:
  - ‚úÖ 86 audios transcritos (Paula chat)
  - ‚úÖ Accuracy > 90% (spot-check 10 audios)
  - ‚úÖ Cost < $2/chat
  - ‚úÖ Cache hit rate > 70%
  - ‚úÖ NO budget overruns
```

### 6.2. Roadmap v2.0 (whisper.cpp Local)

```yaml
Phase 2 (v2.0) - Local Whisper for Real-Time:
  Scope: Foundation para Bit√°cora Conversacional
  Tasks:
    1. Research whisper-rs bindings (vs whispercpp)
    2. Model management (download, cache models)
    3. GPU detection (CUDA/ROCm)
    4. whisper context initialization
    5. Audio capture (microphone input)
    6. Voice Activity Detection (VAD)
    7. Real-time transcription (<1s)
    8. Streaming output
    9. Integration con ShuiDao Router
    10. TTS integration (voice responses)
  
  Testing:
    - Latency benchmarks (GPU vs CPU)
    - Accuracy comparison (vs Whisper API)
    - Stress test (continuous conversation)
    - Multi-language support
    - Background noise handling
  
  Effort: 2-3 semanas
  Priority: LOW (v2.0, strategic but NOT MVP blocker)
  Blockers: GPU setup, C++ FFI complexity

Success Criteria:
  - ‚úÖ Latency < 1s (GPU) or < 2s (CPU)
  - ‚úÖ Accuracy > 90%
  - ‚úÖ FREE ongoing (no API costs)
  - ‚úÖ Natural conversation flow
  - ‚úÖ Privacy-first (offline)
```

### Dependencies

```yaml
v1.0 (Whisper API):
  Already Exists:
    ‚úÖ LLMClient (HTTP client pattern reusable)
    ‚úÖ ContentCache (SQLite-based)
    ‚úÖ BudgetTracker (from ImageAnalyzer)
  
  Need to Create:
    - src/data_import/content_extractors/audio.rs
    - examples/test_audio_transcriber.rs
  
  External:
    - OpenAI Whisper API access ($0.006/min)
    - reqwest crate (multipart uploads)

v2.0 (whisper.cpp):
  Need to Create:
    - src/sensory_engine/audio_transcriber.rs
    - src/sensory_engine/vad.rs (Voice Activity Detection)
    - Model management system
  
  External:
    - whisper-rs crate (Rust bindings)
    - whisper.cpp models (75MB-1.5GB)
    - GPU setup (CUDA/ROCm)
    - Audio capture (cpal crate)
```

---

## 7. VALIDACI√ìN

### 7.1. Test Cases v1.0 (Whisper API)

```yaml
Test 1 - Basic Transcription:
  Input: test_data/audio_nota_de_voz.opus (2 min, Spanish)
  Expected Output:
    text: "Hola, te cuento que hoy fue un d√≠a incre√≠ble..."
    language: "es"
    duration: 120.5
    cost: $0.72
    segments: [...timestamps...]

Test 2 - Short Audio:
  Input: test_data/audio_rapido.opus (10 seg)
  Expected Output:
    text: "Ll√°mame cuando puedas"
    language: "es"
    duration: 10.0
    cost: $0.06

Test 3 - Long Audio:
  Input: test_data/audio_largo.opus (12 min)
  Expected Output:
    text: "D√©jame explicarte el proyecto..."
    duration: 720.0
    cost: $4.32
    Alert: "High cost, 86% of daily budget"

Test 4 - Cache Hit:
  Input: test_data/audio_nota_de_voz.opus (same as Test 1)
  Expected:
    - Cache HIT
    - NO API call
    - Cost: $0 (cached)

Test 5 - Budget Exceeded:
  Scenario: Daily spending at $5.00
  Input: test_data/audio_extra.opus (5 min)
  Expected:
    - Budget check FAILS
    - Skip transcription
    - Alert: "Daily budget exceeded"

Test 6 - Multi-Language:
  Input: test_data/audio_english.mp3 (2 min, English)
  Expected Output:
    text: "Hey, how are you doing..."
    language: "en"
    duration: 120.0
```

### 7.2. Test Cases v2.0 (whisper.cpp)

```yaml
Test 1 - Real-Time Latency:
  Input: Microphone audio (1 second speech)
  Expected:
    - Transcription starts < 100ms after speech ends
    - Full result < 1s (GPU) or < 2s (CPU)

Test 2 - Continuous Conversation:
  Input: 5 minutes continuous conversation
  Expected:
    - NO lag accumulation
    - Consistent < 1s latency
    - NO memory leaks

Test 3 - Background Noise:
  Input: Speech with background music/noise
  Expected:
    - Accuracy > 85% (acceptable degradation)
    - NO false positives (transcribing noise)

Test 4 - Multi-Speaker:
  Input: Conversation between 2 people
  Expected:
    - Transcribe both speakers
    - Optional: Speaker diarization (future)
```

### Performance Benchmarks

```yaml
v1.0 (Whisper API):
  Metrics:
    - API latency: < 5 segundos
    - Accuracy: > 95% (Whisper benchmark)
    - Cache hit rate: > 70%
    - Cost per audio: ~$0.012 (2 min avg)
    - Cost per chat: ~$1.03 (86 audios)
  
  Budget:
    - Daily limit: $5/day
    - Alert threshold: $4/day (80%)
    - Fail-safe: Skip if exceeded

v2.0 (whisper.cpp):
  Metrics:
    - Latency (GPU): < 1s
    - Latency (CPU): < 2s
    - Accuracy: > 90%
    - Memory usage: ~500MB-2GB (model dependent)
    - CPU usage: ~20-40% (1 core)
```

---

## 8. IMPLEMENTACI√ìN RUST

### 8.1. v1.0 AudioTranscriber (Whisper API)

```rust
// src/data_import/content_extractors/audio.rs

use reqwest::multipart::{Form, Part};
use serde::{Deserialize, Serialize};
use std::path::Path;
use tokio::fs;

/// Audio transcriber using OpenAI Whisper API
pub struct AudioTranscriber {
    /// HTTP client
    client: reqwest::Client,
    
    /// OpenAI API key
    api_key: String,
    
    /// Cache (30 d√≠as)
    cache: ContentCache,
    
    /// Budget tracker ($5/day)
    budget: BudgetTracker,
}

impl AudioTranscriber {
    pub fn new(api_key: String, cache: ContentCache, budget: BudgetTracker) -> Self {
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(60))  // Longer timeout for audio
            .build()
            .expect("Failed to create HTTP client");
        
        Self {
            client,
            api_key,
            cache,
            budget,
        }
    }
    
    /// Transcribe audio file using Whisper API
    pub async fn transcribe(&self, audio_path: &Path) -> Result<AudioTranscription> {
        // Generate cache key
        let cache_key = self.cache_key(audio_path).await?;
        
        // Check cache
        if let Some(cached) = self.cache.get(&cache_key).await? {
            tracing::info!("Cache HIT: {:?}", audio_path);
            return Ok(cached);
        }
        
        tracing::info!("Cache MISS: {:?}", audio_path);
        
        // Get audio duration (for cost estimation)
        let duration = self.estimate_duration(audio_path).await?;
        let estimated_cost = (duration / 60.0) * 0.006;  // $0.006/min
        
        // Check budget
        if self.budget.would_exceed(estimated_cost).await? {
            tracing::warn!("Budget would be exceeded, skipping transcription");
            return Ok(AudioTranscription::budget_exceeded());
        }
        
        // Call Whisper API
        let transcription = self.call_whisper_api(audio_path).await?;
        
        // Record cost
        let actual_cost = (transcription.duration / 60.0) * 0.006;
        self.budget.record(actual_cost).await?;
        
        // Cache result
        self.cache.set(&cache_key, &transcription, Duration::from_days(30)).await?;
        
        Ok(transcription)
    }
    
    /// Call OpenAI Whisper API
    async fn call_whisper_api(&self, audio_path: &Path) -> Result<AudioTranscription> {
        // Read audio file
        let audio_bytes = fs::read(audio_path).await?;
        
        // Prepare multipart form
        let file_part = Part::bytes(audio_bytes)
            .file_name(audio_path.file_name()
                .unwrap()
                .to_string_lossy()
                .to_string())
            .mime_str("audio/mpeg")?;  // Whisper accepts multiple formats
        
        let form = Form::new()
            .part("file", file_part)
            .text("model", "whisper-1")
            .text("response_format", "verbose_json")  // Includes timestamps
            .text("temperature", "0.0");  // Deterministic
        
        // Make API request
        let response = self.client
            .post("https://api.openai.com/v1/audio/transcriptions")
            .bearer_auth(&self.api_key)
            .multipart(form)
            .send()
            .await
            .map_err(|e| AudioTranscriptionError::APIError {
                details: e.to_string(),
            })?;
        
        // Check status
        if !response.status().is_success() {
            let error_body = response.text().await?;
            return Err(AudioTranscriptionError::APIError {
                details: format!("Status {}: {}", response.status(), error_body),
            });
        }
        
        // Parse response
        let whisper_response: WhisperAPIResponse = response.json().await?;
        
        // Build transcription
        Ok(AudioTranscription {
            text: whisper_response.text,
            language: whisper_response.language,
            duration: whisper_response.duration,
            segments: whisper_response.segments.unwrap_or_default()
                .into_iter()
                .map(|s| TranscriptSegment {
                    text: s.text,
                    start: s.start,
                    end: s.end,
                })
                .collect(),
            transcribed_at: chrono::Utc::now(),
            model: "whisper-1".to_string(),
            cost: (whisper_response.duration / 60.0) * 0.006,
        })
    }
    
    /// Estimate audio duration (seconds)
    /// 
    /// TODO: Use ffprobe for accurate duration
    /// For now, estimate from file size (rough heuristic)
    async fn estimate_duration(&self, path: &Path) -> Result<f64> {
        let metadata = fs::metadata(path).await?;
        let file_size_mb = metadata.len() as f64 / (1024.0 * 1024.0);
        
        // Rough estimate: ~1MB per minute for OPUS/MP3
        let estimated_minutes = file_size_mb;
        
        Ok(estimated_minutes * 60.0)  // Return seconds
    }
    
    /// Generate cache key (path + file hash)
    async fn cache_key(&self, path: &Path) -> Result<String> {
        use sha2::{Sha256, Digest};
        
        let bytes = fs::read(path).await?;
        let mut hasher = Sha256::new();
        hasher.update(path.to_string_lossy().as_bytes());
        hasher.update(&bytes);
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

/// Whisper API response
#[derive(Debug, Deserialize)]
struct WhisperAPIResponse {
    text: String,
    language: String,
    duration: f64,
    segments: Option<Vec<WhisperSegment>>,
}

#[derive(Debug, Deserialize)]
struct WhisperSegment {
    text: String,
    start: f64,
    end: f64,
}

/// Final audio transcription
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AudioTranscription {
    pub text: String,
    pub language: String,
    pub duration: f64,
    pub segments: Vec<TranscriptSegment>,
    pub transcribed_at: chrono::DateTime<chrono::Utc>,
    pub model: String,
    pub cost: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TranscriptSegment {
    pub text: String,
    pub start: f64,
    pub end: f64,
}

impl AudioTranscription {
    /// Placeholder when budget exceeded
    fn budget_exceeded() -> Self {
        Self {
            text: "Transcription skipped (budget exceeded)".to_string(),
            language: "unknown".to_string(),
            duration: 0.0,
            segments: vec![],
            transcribed_at: chrono::Utc::now(),
            model: "none".to_string(),
            cost: 0.0,
        }
    }
}
```

### 8.2. Budget Tracker Enhancement

```rust
// src/data_import/budget_tracker.rs

impl BudgetTracker {
    /// Check if spending + cost would exceed limit
    pub async fn would_exceed(&self, cost: f64) -> Result<bool> {
        let state = self.state.lock().await;
        let today = Utc::now().date_naive();
        
        if state.date != today {
            // New day, budget OK
            Ok(false)
        } else {
            Ok(state.spent + cost > self.limit)
        }
    }
}
```

### 8.3. Integration con HyperlinkExtractor

```rust
// src/data_import/extraction.rs

impl HyperlinkExtractor {
    pub async fn extract(&self, digested: &DigestedData) -> Result<ExtractedNutrients> {
        // ... existing code ...
        
        // Extract audio attachments
        let audio_attachments = self.extract_audio_attachments(digested)?;
        
        for audio in audio_attachments {
            // Transcribe audio
            let transcription = self.audio_transcriber.transcribe(&audio.path).await?;
            
            // Convert to nutrients
            let nutrients = self.audio_to_nutrients(transcription, &audio);
            all_nutrients.extend(nutrients);
        }
        
        Ok(all_nutrients)
    }
    
    /// Convert AudioTranscription to nutrients
    fn audio_to_nutrients(&self, transcription: AudioTranscription, audio: &AudioAttachment) -> Vec<Nutrient> {
        let mut nutrients = Vec::new();
        
        // Main transcript nutrient
        nutrients.push(Nutrient {
            dimension: "AUDIO_CONTENT".to_string(),
            key: format!("audio_transcript_{}", audio.filename),
            value: transcription.text.clone(),
            confidence: 0.95,  // Whisper accuracy
            source: "whisper_api".to_string(),
            context: Some(format!(
                "Audio: {} ({:.1}s, {})",
                audio.filename, transcription.duration, transcription.language
            )),
        });
        
        // Language nutrient
        nutrients.push(Nutrient {
            dimension: "LINGUISTIC".to_string(),
            key: format!("audio_language_{}", audio.filename),
            value: transcription.language.clone(),
            confidence: 0.90,
            source: "whisper_api".to_string(),
            context: Some(format!("Detected from audio: {}", audio.filename)),
        });
        
        // Duration metadata (for temporal analysis)
        nutrients.push(Nutrient {
            dimension: "TEMPORAL".to_string(),
            key: format!("audio_duration_{}", audio.filename),
            value: format!("{:.1}s", transcription.duration),
            confidence: 1.0,
            source: "whisper_api".to_string(),
            context: Some(format!("Audio length: {}", audio.filename)),
        });
        
        // TODO v2.0: Emotion detection from voice tone
        // nutrients.push(analyze_voice_emotion(&transcription));
        
        nutrients
    }
}
```

### 8.4. v2.0 Preview (whisper.cpp Local)

```rust
// src/sensory_engine/audio_transcriber.rs

use whisper_rs::{WhisperContext, FullParams, SamplingStrategy};

/// Local audio transcriber using whisper.cpp
pub struct LocalAudioTranscriber {
    /// Whisper context (model loaded)
    ctx: WhisperContext,
    
    /// Inference parameters
    params: FullParams,
}

impl LocalAudioTranscriber {
    /// Initialize with model
    pub fn new(model_path: &str) -> Result<Self> {
        // Load model
        let ctx = WhisperContext::new(model_path)
            .map_err(|e| AudioTranscriptionError::ModelLoadFailed {
                path: model_path.to_string(),
                reason: e.to_string(),
            })?;
        
        // Configure parameters
        let mut params = FullParams::new(SamplingStrategy::Greedy { best_of: 1 });
        params.set_language(Some("es"));
        params.set_print_special(false);
        params.set_print_progress(false);
        params.set_print_realtime(false);
        params.set_print_timestamps(false);
        
        tracing::info!("Whisper local model loaded: {}", model_path);
        
        Ok(Self { ctx, params })
    }
    
    /// Transcribe audio samples (real-time)
    pub fn transcribe_samples(&mut self, samples: &[f32]) -> Result<String> {
        // Run inference
        self.ctx
            .full(self.params.clone(), samples)
            .map_err(|e| AudioTranscriptionError::InferenceFailed {
                reason: e.to_string(),
            })?;
        
        // Get transcript
        let num_segments = self.ctx.full_n_segments();
        let mut transcript = String::new();
        
        for i in 0..num_segments {
            if let Ok(segment) = self.ctx.full_get_segment_text(i) {
                transcript.push_str(&segment);
                transcript.push(' ');
            }
        }
        
        Ok(transcript.trim().to_string())
    }
}

// TODO v2.0: Full implementation with VAD, audio capture, streaming
```

---

## 9. EST√ÅNDARES

### 9.1. Error Handling

```rust
#[derive(Debug, thiserror::Error)]
pub enum AudioTranscriptionError {
    #[error("Failed to read audio file {path:?}: {reason}")]
    ReadFailed {
        path: PathBuf,
        reason: String,
    },
    
    #[error("Whisper API error: {details}")]
    APIError {
        details: String,
    },
    
    #[error("Failed to parse API response: {details}")]
    ParseError {
        details: String,
    },
    
    #[error("Daily budget exceeded (${limit})")]
    BudgetExceeded {
        limit: f64,
    },
    
    #[error("Failed to load whisper model {path}: {reason}")]
    ModelLoadFailed {
        path: String,
        reason: String,
    },
    
    #[error("Whisper inference failed: {reason}")]
    InferenceFailed {
        reason: String,
    },
}
```

### 9.2. Logging

```rust
// Startup
tracing::info!("AudioTranscriber initialized (Whisper API, $5/day budget)");

// Per-audio logging
tracing::info!("Transcribing audio: {:?} ({:.1}s estimated)", path, duration);
tracing::info!("Cache {}: {:?}", if hit { "HIT" } else { "MISS" }, path);
tracing::info!("Budget: ${:.2} / $5.00 today (cost: ${:.2})", spent, cost);
tracing::info!("Transcription complete: {} chars, {} language", text.len(), lang);

// Error logging
tracing::error!("Whisper API failed: {}", error);
tracing::warn!("Budget exceeded, skipping audio transcription");
```

### 9.3. Privacy Guidelines

```yaml
Audio Storage:
  - Original audio files: NEVER stored in Bit√°cora DB
  - Transcriptions: Stored (text only, NO audio)
  - Audio sent to OpenAI (v1.0): YES (Whisper API)
  - Audio processed locally (v2.0): YES (whisper.cpp)

User Control:
  - Setting: transcribe_audio (default: true)
  - Setting: use_local_whisper (default: false, v2.0)
  - Setting: delete_audio_after_transcription (default: true)

PII in Transcripts:
  - Transcripts may contain names, locations, sensitive info
  - Apply same privacy rules as text messages
  - Optional: PII detection/redaction (v2.0)
```

---

## 10. CHECKLIST

### Pre-Implementation v1.0

- [ ] Read GUIA.md (Theremin philosophy)
- [ ] Read METOD_DOCS.md (methodology)
- [ ] Get OpenAI API key (Whisper access)
- [ ] Test Whisper API with sample audio
- [ ] Estimate cost for Paula chat (86 audios x ~2 min avg)

### Phase 1 (v1.0) - Whisper API Integration

- [ ] Create `src/data_import/content_extractors/audio.rs`
- [ ] Implement `AudioTranscriber` struct
- [ ] Add `reqwest` multipart dependency
- [ ] Implement `call_whisper_api()` (multipart upload)
- [ ] Implement cache layer (30 d√≠as)
- [ ] Enhance `BudgetTracker` (would_exceed check)
- [ ] Add error handling (`AudioTranscriptionError`)
- [ ] Add logging (tracing)
- [ ] Implement `estimate_duration()` (file size heuristic)
- [ ] Create `examples/test_audio_transcriber.rs`
- [ ] Test con 10 audios de Paula chat
- [ ] Validate transcription accuracy (manual check >90%)
- [ ] Validate language detection
- [ ] Benchmark cost ($0.006/min expected)
- [ ] Test cache hits
- [ ] Test budget limit ($5/day)
- [ ] Integration con `HyperlinkExtractor`
- [ ] Test end-to-end (WhatsApp import ‚Üí nutrients)
- [ ] Update `CHECKLIST_V2.md` (Task 7.x.3.10: AudioTranscriber)
- [ ] Git commit: "feat(extraction): AudioTranscriber v1.0 - Whisper API"

### Phase 2 (v2.0) - whisper.cpp Local (Future)

- [ ] Research whisper-rs vs whispercpp crates
- [ ] Download whisper models (tiny/base/small/medium)
- [ ] Implement model management system
- [ ] GPU detection (CUDA/ROCm)
- [ ] Create `src/sensory_engine/audio_transcriber.rs`
- [ ] Implement `LocalAudioTranscriber` struct
- [ ] Implement real-time transcription (<1s)
- [ ] Implement Voice Activity Detection (VAD)
- [ ] Audio capture (microphone input)
- [ ] Streaming output
- [ ] Latency benchmarks (GPU vs CPU)
- [ ] Accuracy comparison (vs Whisper API)
- [ ] Integration con ShuiDao Router
- [ ] TTS integration (voice responses)
- [ ] User documentation (GPU setup)
- [ ] Update CHECKLIST_V2.md (Phase 9.x: Conversational Interface)

---

## üìö REFERENCIAS

### Documentos Relacionados

- `ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md` - Parent document
- `ROADMAP_V2/02_COMPONENTES/18.1_hyperlink-content-extraction.md` - Multimedia extraction
- `ROADMAP_V2/02_COMPONENTES/18.2_image-recognition-analysis.md` - Image vision
- `ROADMAP_V2/02_COMPONENTES/01_sensory-engine.md` - AudioTranscriber STUB
- `ROADMAP_V2/CHECKLIST_V2.md` - Phase 9.3: Audio transcription

### External Resources

- [OpenAI Whisper API Docs](https://platform.openai.com/docs/guides/speech-to-text)
- [whisper.cpp GitHub](https://github.com/ggerganov/whisper.cpp)
- [whisper-rs Rust bindings](https://github.com/tazz4843/whisper-rs)
- [Whisper paper (OpenAI)](https://cdn.openai.com/papers/whisper.pdf)

### Cost Calculators

```yaml
Whisper API Pricing:
  Model: whisper-1
  Cost: $0.006 / minute
  
  Examples:
    - 1 minute: $0.006
    - 10 minutes: $0.06
    - 1 hour: $0.36
    - Paula chat (172 min): $1.03
    - Daily limit ($5): ~833 minutes (13.9 hours)
```

---

## üéØ DECISI√ìN ESTRAT√âGICA

```yaml
v1.0: Whisper API (Batch Processing)
  Context: WhatsApp imports (historical data)
  Cost: $1-2/chat one-time (acceptable)
  Latency: ~3-5s (OK para batch)
  Accuracy: 95%+ (critical para content)
  Implementation: Simple HTTP API (3-4 d√≠as)
  NO MVP blocker: YES
  
v2.0: whisper.cpp Local (Real-Time Conversation)
  Context: Bit√°cora Conversacional (daily usage)
  Cost: $0/month (FREE ongoing after setup)
  Latency: <1s (GPU), <2s (CPU)
  Accuracy: 90-93% (acceptable para conversaci√≥n)
  Implementation: Complex FFI + GPU setup (2-3 semanas)
  Strategic Foundation: YES (voice interface future)
  
Philosophy:
  "v1.0: Accuracy first, content extraction.
   v2.0: Natural flow, conversational interface.
   Hybrid approach = Best of both worlds."
```

**Key Insight:** Audio transcription es DUAL NATURE:
- v1.0: Content extraction (batch, accurate, simple)
- v2.0: Conversational interface (real-time, natural, complex)

**NO intentar resolver ambos en v1.0. Phase approach strategic.**
