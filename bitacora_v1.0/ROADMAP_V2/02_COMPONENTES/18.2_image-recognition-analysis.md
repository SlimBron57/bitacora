```yaml
# === DATOS DE AUDITOR√çA ===
Archivo: ROADMAP_V2/02_COMPONENTES/18.2_image-recognition-analysis.md
Versi√≥n: 1.0
Fecha Creaci√≥n: 2025-11-29
√öltima Actualizaci√≥n: 2025-11-29 03:30:00
Autor: Sistema Bit√°cora (B) + Eduardo
Prop√≥sito: Image recognition & analysis para WhatsApp imports (v1.0 lightweight approach)
Estado: ACTIVO - Dise√±o estrat√©gico
Parent Doc: 18_hyperlink-extractor.md (HyperlinkExtractor)
Inspiraci√≥n: GPT-4o Vision, Claude Sonnet Vision, Gemini Vision, LLaVA
Filosof√≠a: "No reinventar Computer Vision. Delegar a Vision LLMs (lightweight, almost free)."
Cambios: Initial creation - strategic vision + implementation plan
Punto de Entrada: Secci√≥n 1 (Preludio) ‚Üí Secci√≥n 3 (Filosof√≠a) ‚Üí Secci√≥n 6 (Plan de Acci√≥n)
Related: 18.1_hyperlink-content-extraction.md (multimedia), 18.3_voice-to-text-transcription.md (audio)
# === FIN DATOS DE AUDITOR√çA ===
```

# üñºÔ∏è Image Recognition & Analysis

> **"Una imagen vale m√°s que mil palabras. Pero solo si la ENTIENDES."**

---

## üìë √çNDICE

1. [Preludio: El Problema Humano](#1-preludio-el-problema-humano)
2. [Inventario F√≠sico](#2-inventario-f√≠sico)
3. [Filosof√≠a: Por Qu√© Vision LLMs](#3-filosof√≠a-por-qu√©-vision-llms)
4. [Prop√≥sito del M√≥dulo](#4-prop√≥sito-del-m√≥dulo)
5. [Flujo L√≥gico](#5-flujo-l√≥gico)
6. [Plan de Acci√≥n](#6-plan-de-acci√≥n)
7. [Validaci√≥n](#7-validaci√≥n)
8. [Implementaci√≥n Rust](#8-implementaci√≥n-rust)
9. [Est√°ndares](#9-est√°ndares)
10. [Checklist](#10-checklist)

---

## 1. PRELUDIO: EL PROBLEMA HUMANO

### La Historia de las 406 Fotos

Paula Roque chat: **1,354 mensajes, 406 fotos compartidas** (30% del chat).

Eduardo mira esas 406 fotos:
- Screenshot de conversaci√≥n con amiga
- Meme gracioso sobre pol√≠tica
- Foto de su gato "Mishka"
- Selfie en la playa (cumplea√±os)
- Captura de mapa (ubicaci√≥n de restaurante)
- Foto de libro que est√° leyendo
- Ticket de concierto
- Infograf√≠a sobre salud mental
- Recibo bancario (cuidado, PII)

**La pregunta NO es:** "¬øC√≥mo extraigo pixels?"

**La pregunta ES:** "¬øQu√© SIGNIFICA cada foto en el contexto de su vida?"

### El Dilema: Computer Vision vs Vision LLMs

```yaml
Approach Tradicional (Computer Vision):
  Tools: OpenCV, TensorFlow, PyTorch, YOLO, ResNet
  Pros:
    - R√°pido (< 100ms inference)
    - Offline (no API costs)
    - Control total
  
  Contras:
    - Requiere modelos pesados (500MB-2GB)
    - Entrenamiento complejo (datasets, GPUs)
    - Features limitados: object detection, face recognition, OCR
    - NO entiende CONTEXTO ("gato" vs "gato de Paula llamado Mishka")
    - NO entiende SEM√ÅNTICA (meme pol√≠tico vs foto casual)
    - Mantenimiento: modelos se desactualizan

Approach Vision LLMs:
  Tools: GPT-4o Vision, Claude Sonnet Vision, Gemini Vision, LLaVA
  Pros:
    - Entiende CONTEXTO sem√°ntico completo
    - Responde preguntas: "¬øQu√© emoci√≥n transmite?"
    - Multimodal: imagen + texto prompt
    - Zero training (models ya entrenados)
    - Descripci√≥n natural: "Selfie de mujer sonriente en playa al atardecer"
    - OCR impl√≠cito (lee texto en screenshots)
    - Privacy-aware: detecta PII (caras, IDs, tarjetas)
  
  Contras:
    - Costo: ~$0.003-0.01 por imagen (GPT-4o Vision)
    - Latencia: ~2-4 segundos
    - Requiere internet
    - Privacy: imagen va a OpenAI (mitigable con cache)

Decisi√≥n: ‚úÖ Vision LLMs (GPT-4o Vision)
  Why:
    - v1.0: WhatsApp imports (NOT real-time)
    - 406 fotos x $0.005 = $2.03 (ONE-TIME costo por chat)
    - 30-day cache ‚Üí NO re-pagamos por misma foto
    - Context-awareness >> speed para memoria personal
    - NO necesitamos Computer Vision pipeline complejo
```

---

## 2. INVENTARIO F√çSICO

### Archivos Relacionados

```bash
# Existing files (referencing images)
ROADMAP_V2/CHECKLIST_V2.md              # "406 photos" in Paula chat
ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md  # Attachment detection
ROADMAP_V2/02_COMPONENTES/18.1_hyperlink-content-extraction.md  # Multimedia extraction

# Files to create
src/data_import/content_extractors/image.rs          # ImageAnalyzer
src/data_import/extraction.rs                        # HyperlinkExtractor integration
examples/test_image_analyzer.rs                      # Testing
```

### Dependencias

```toml
[dependencies]
# Vision LLM integration
# (LLMClient already exists - supports Vision)

# Image processing
image = "0.24"                    # Load, resize, format conversion
base64 = "0.21"                   # Encode images for API
tokio = { version = "1", features = ["fs"] }  # Async file I/O

# Optional (if we do local preprocessing)
# opencv = "0.88"                 # NOT needed (delegating to LLM)
# tensorflow = "0.21"             # NOT needed (delegating to LLM)
```

**Filosof√≠a:** Minimal dependencies. Leverage Vision LLMs for intelligence.

---

## 3. FILOSOF√çA: POR QU√â VISION LLMS

### Principios de Dise√±o

**1. Delegar a Quien Lo Hace Mejor**

Computer Vision es un campo COMPLEJO:
- Object detection: YOLO, Faster R-CNN
- Face recognition: FaceNet, ArcFace
- OCR: Tesseract, EasyOCR
- Scene understanding: ResNet, Vision Transformers

**Vision LLMs ya hacen TODO eso. Y mejor.**

```yaml
GPT-4o Vision capabilities:
  - Object detection: "Detecta objetos en imagen"
  - OCR: "Extrae texto de screenshot"
  - Scene understanding: "¬øD√≥nde fue tomada esta foto?"
  - Emotion recognition: "¬øQu√© emoci√≥n transmite?"
  - Context: "¬øEs formal o casual?"
  - Privacy detection: "¬øContiene informaci√≥n sensible?"
  - Relationships: "¬øCu√°ntas personas hay? ¬øRelaci√≥n?"
```

**2. Context Over Pixels**

Bit√°cora NO es un sistema de Computer Vision.  
Bit√°cora es un sistema de MEMORIA PERSONAL.

NO necesitamos:
- Bounding boxes precisos
- Clasificaci√≥n de 1000 objetos ImageNet
- Detecci√≥n de 68 facial landmarks

Necesitamos:
- **Descripci√≥n sem√°ntica:** "Selfie de Paula en playa, sonriente, atardecer"
- **Contexto emocional:** "Foto casual, momento feliz"
- **Relevancia:** "Relacionado con viaje a Bahamas (temporal pattern)"
- **Privacy:** "NO contiene informaci√≥n sensible"

**3. Cost-Effective para v1.0**

```yaml
Real Cost Analysis (GPT-4o Vision):
  Pricing (Nov 2025):
    Input (image): $0.003 per image (low res)
    Input (image): $0.01 per image (high res)
    Output (tokens): $0.600 / 1M tokens
  
  Average Analysis:
    Image: Low res (512x512) - $0.003
    Prompt: 50 tokens - $0.0000075
    Response: 200 tokens - $0.00012
    Total: ~$0.0032 per image ‚âà $0.005
  
  Real Usage (Paula chat):
    406 photos x $0.005 = $2.03 (ONE-TIME)
    Cache: 30 days (images don't change)
    Re-analysis: NEVER (unless user re-shares)
  
  Scaling (100 users):
    Average: 200 photos/user/import
    20,000 photos x $0.005 = $100 (one-time per user)
    Incremental: ~5-10 new photos/user/day
    500-1000 photos/day x $0.005 = $2.50-5/day
    Monthly: $75-150/month
  
  vs Computer Vision Pipeline:
    Developer time: 60+ hours (models, training, integration)
    Infrastructure: GPU instance ($200-500/month)
    Maintenance: 10+ hours/month (model updates)
    
  Conclusion: Vision LLMs ~100x cheaper considerando developer time
```

**4. Privacy-Aware**

Vision LLMs pueden detectar PII:
- Caras (blur opcional)
- Documentos de identidad
- Tarjetas de cr√©dito
- Informaci√≥n bancaria
- Direcciones

**Prompt:** "¬øEsta imagen contiene informaci√≥n sensible (caras, IDs, tarjetas)?"

---

## 4. PROP√ìSITO DEL M√ìDULO

### Objetivo Primario

**Analizar im√°genes compartidas en WhatsApp imports para extraer:**
1. **Descripci√≥n sem√°ntica** (qu√© ES la imagen)
2. **Contexto emocional** (qu√© transmite)
3. **Informaci√≥n extra√≠da** (texto OCR si hay)
4. **Privacy metadata** (contiene PII?)
5. **Clasificaci√≥n** (meme, selfie, screenshot, document, etc.)

### Audiencia

- Eduardo (dise√±o estrat√©gico)
- Desarrolladores Rust (implementaci√≥n)
- HyperlinkExtractor (consumidor)

### Resultado Esperado

Despu√©s de implementar este m√≥dulo:
- ‚úÖ **406 fotos** de Paula Roque chat analizadas
- ‚úÖ **Descripci√≥n natural** para cada foto
- ‚úÖ **Privacy detection** (blur faces si necesario)
- ‚úÖ **Context-aware nutrients** (relacionados con EmotionalSpace, TemporalAnalyzer)

---

## 5. FLUJO L√ìGICO

### Architecture Overview

```
WhatsApp Import (1,354 messages)
    ‚Üì
DigestedData (406 attachments detected)
    ‚Üì
HyperlinkExtractor (classify attachments)
    ‚Üì
ImageAnalyzer (Vision LLM delegation) ‚Üê YOU ARE HERE
    ‚Üì
ImageNutrients
    {
      description: "Selfie de Paula en playa, sonriente, atardecer",
      emotions: ["happiness", "relaxation"],
      ocr_text: null,
      objects: ["person", "ocean", "sunset"],
      scene: "outdoor, beach",
      contains_pii: false,
      image_type: "selfie",
      context_tags: ["social", "vacation", "personal"],
      timestamp_context: "Correlacionado con viaje a Bahamas",
    }
    ‚Üì
ExtractedNutrients (integration con otros extractors)
```

### Flujo Detallado

```yaml
Phase 1 - Detect Image Attachments:
  Input: DigestedData (WhatsApp import)
  Process:
    - Scan for <attached: filename.jpg> markers
    - Classify by MIME type: image/jpeg, image/png, image/webp
    - Extract file path from import directory
  Output: List<ImagePath>

Phase 2 - Load & Preprocess:
  Input: ImagePath
  Process:
    - Load image with `image` crate
    - Resize if > 2048x2048 (Vision API limits)
    - Convert to JPEG if PNG/WEBP (compression)
    - Encode to base64
  Output: base64_image

Phase 3 - Vision LLM Analysis:
  Input: base64_image
  Prompt:
    """
    Analyze this image and extract structured information.
    
    Return JSON:
    {
      "description": "Natural language description (1-2 sentences)",
      "image_type": "selfie/screenshot/meme/document/photo/other",
      "scene": "indoor/outdoor/unknown",
      "objects": ["object1", "object2", "object3"],
      "people_count": 0,
      "emotions": ["emotion1", "emotion2"],
      "ocr_text": "Text found in image (or null)",
      "contains_pii": false,
      "pii_types": ["faces", "ids", "credit_cards"],
      "context_tags": ["tag1", "tag2", "tag3"],
      "is_meme": false,
      "quality": "high/medium/low"
    }
    
    Guidelines:
    - Be concise but descriptive
    - Detect PII (faces, documents, cards)
    - Extract ALL text visible (OCR)
    - Classify image type accurately
    - Infer emotional context
    """
  
  Model: gpt-4o (vision capability)
  Max tokens: 500 (output)
  Temperature: 0.1 (deterministic)
  Response format: JSON

Phase 4 - Post-Processing:
  - Parse JSON response
  - Validate structure
  - Add metadata:
    * analyzed_at: timestamp
    * model: "gpt-4o-vision"
    * cost: $0.005
  - Correlate con EmotionalSpace:
    * "happiness" emotion ‚Üí Positive valence
    * "beach" scene ‚Üí Vacation context
  - Correlate con TemporalAnalyzer:
    * Image timestamp vs message timestamp
    * Pattern: "Paula comparte fotos de vacaciones domingos"

Phase 5 - Cache & Return:
  - Cache result (30 d√≠as, images don't change)
  - Key: SHA256(image_path + image_hash)
  - Return ImageNutrients

Phase 6 - Privacy Protection (Optional v2.0):
  - If contains_pii AND user opted for privacy:
    * Blur faces (opencv)
    * Redact text (OCR + masking)
    * Store blurred version
```

---

## 6. PLAN DE ACCI√ìN

### Roadmap de Implementaci√≥n

```yaml
Phase 1 (v1.0) - Vision LLM Basic:
  Scope: GPT-4o Vision analysis b√°sico
  Tasks:
    1. Create ImageAnalyzer struct
    2. Integrate LLMClient (vision capability)
    3. Image loading + resize + base64 encoding
    4. Vision prompt engineering
    5. JSON parsing + validation
    6. Cache layer (30 d√≠as)
    7. Cost tracking ($5/day budget)
  
  Testing:
    - Test con 10 fotos de Paula chat
    - Validate descriptions make sense
    - Check PII detection accuracy
    - Benchmark cost ($0.005/image expected)
  
  Effort: 3-4 d√≠as
  Priority: MEDIUM (v1.0 feature, enhances nutrients)
  Blockers: None (LLMClient exists, Vision API available)

Phase 2 (v1.1) - Context Correlation:
  Scope: Correlate image analysis con otros extractors
  Tasks:
    1. Emotional correlation (image emotions ‚Üí EmotionalSpace)
    2. Temporal patterns (vacation photos on weekends)
    3. Relationship detection (photos con quien)
    4. Topic extraction (beach ‚Üí travel, book ‚Üí literature)
  
  Effort: 2-3 d√≠as
  Priority: MEDIUM

Phase 3 (v2.0) - Privacy Enhancement:
  Scope: Face blurring, PII redaction
  Tasks:
    1. opencv integration (face detection)
    2. Gaussian blur for faces
    3. OCR + text masking (credit cards, IDs)
    4. Store blurred versions
  
  Effort: 1 semana
  Priority: LOW (v2.0, privacy-focused users)
```

### Dependencies

```yaml
Already Exists:
  ‚úÖ LLMClient (src/multi_agent/llm_client.rs) - supports Vision APIs
  ‚úÖ ContentCache (SQLite-based)
  ‚úÖ HyperlinkExtractor skeleton

Need to Create:
  - src/data_import/content_extractors/image.rs (ImageAnalyzer)
  - examples/test_image_analyzer.rs (testing)
  - Vision prompt templates

External:
  - GPT-4o Vision API access (OpenAI)
  - `image` crate (resize, format conversion)
  - `base64` crate (encoding)
```

---

## 7. VALIDACI√ìN

### Test Cases

```yaml
Test 1 - Basic Image Analysis:
  Input: test_data/selfie_beach.jpg
  Expected Output:
    description: "Selfie of woman smiling at beach during sunset"
    image_type: "selfie"
    scene: "outdoor"
    objects: ["person", "ocean", "sunset"]
    emotions: ["happiness", "relaxation"]
    contains_pii: true
    pii_types: ["faces"]

Test 2 - Screenshot with Text:
  Input: test_data/whatsapp_screenshot.jpg
  Expected Output:
    description: "WhatsApp conversation screenshot"
    image_type: "screenshot"
    ocr_text: "Hey, how are you?\nI'm good, thanks!"
    contains_pii: true
    pii_types: ["faces", "names"]

Test 3 - Meme:
  Input: test_data/political_meme.jpg
  Expected Output:
    description: "Political meme with humorous caption"
    image_type: "meme"
    is_meme: true
    context_tags: ["humor", "politics", "viral"]
    contains_pii: false

Test 4 - Document:
  Input: test_data/bank_receipt.jpg
  Expected Output:
    description: "Bank receipt or financial document"
    image_type: "document"
    contains_pii: true
    pii_types: ["financial_info", "names"]
    ocr_text: "Receipt #12345..."

Test 5 - Cache Hit:
  Input: test_data/selfie_beach.jpg (same as Test 1)
  Expected:
    - Cache HIT
    - NO API call
    - Cost: $0
```

### Performance Benchmarks

```yaml
Metrics:
  - Latency per image: < 4 segundos
  - Cache hit rate: > 70% (users re-share images)
  - Cost per image: ~$0.005
  - Accuracy (manual validation): > 90%
  - Privacy detection: > 95% (faces, IDs)

Budget:
  - Daily limit: $5/day (1000 images/day)
  - Alert if: > $4/day (80% threshold)
  - Fail-safe: Skip analysis if budget exceeded
```

---

## 8. IMPLEMENTACI√ìN RUST

### 8.1. ImageAnalyzer Core

```rust
// src/data_import/content_extractors/image.rs

use crate::multi_agent::llm_client::LLMClient;
use image::{ImageFormat, DynamicImage};
use serde::{Deserialize, Serialize};
use std::path::Path;
use base64::{Engine as _, engine::general_purpose};

/// Image analyzer using Vision LLMs (GPT-4o)
pub struct ImageAnalyzer {
    /// LLM client with vision capability
    llm: LLMClient,
    
    /// Cache (30 d√≠as - images don't change)
    cache: ContentCache,
    
    /// Max image dimension (resize if larger)
    max_dimension: u32,
    
    /// Daily budget tracker
    budget: BudgetTracker,
}

impl ImageAnalyzer {
    pub fn new(llm: LLMClient, cache: ContentCache) -> Self {
        Self {
            llm,
            cache,
            max_dimension: 2048,  // GPT-4o Vision max
            budget: BudgetTracker::new(5.0),  // $5/day
        }
    }
    
    /// Analyze image using Vision LLM
    pub async fn analyze(&self, image_path: &Path) -> Result<ImageAnalysis> {
        // Check budget first
        if self.budget.is_exceeded().await? {
            tracing::warn!("Daily budget exceeded, skipping image analysis");
            return Ok(ImageAnalysis::budget_exceeded());
        }
        
        // Generate cache key (path + file hash)
        let cache_key = self.cache_key(image_path).await?;
        
        // Check cache
        if let Some(cached) = self.cache.get(&cache_key).await? {
            tracing::info!("Cache HIT: {:?}", image_path);
            return Ok(cached);
        }
        
        tracing::info!("Cache MISS: {:?}", image_path);
        
        // Load and preprocess image
        let base64_image = self.load_and_encode(image_path).await?;
        
        // Vision LLM analysis
        let analysis = self.analyze_with_llm(&base64_image).await?;
        
        // Track cost
        let cost = 0.005;  // ~$0.005 per image
        self.budget.record(cost).await?;
        
        // Cache result (30 d√≠as)
        self.cache.set(&cache_key, &analysis, Duration::from_days(30)).await?;
        
        Ok(analysis)
    }
    
    /// Load image, resize, convert to base64
    async fn load_and_encode(&self, path: &Path) -> Result<String> {
        // Load image
        let img = image::open(path)
            .map_err(|e| ImageAnalysisError::LoadFailed {
                path: path.to_path_buf(),
                reason: e.to_string(),
            })?;
        
        // Resize if too large
        let img = self.resize_if_needed(img);
        
        // Convert to JPEG (compression)
        let mut buffer = Vec::new();
        img.write_to(&mut std::io::Cursor::new(&mut buffer), ImageFormat::Jpeg)
            .map_err(|e| ImageAnalysisError::EncodeFailed {
                reason: e.to_string(),
            })?;
        
        // Encode to base64
        let base64_str = general_purpose::STANDARD.encode(&buffer);
        
        Ok(base64_str)
    }
    
    /// Resize if dimension > max_dimension
    fn resize_if_needed(&self, img: DynamicImage) -> DynamicImage {
        let (width, height) = (img.width(), img.height());
        let max_dim = width.max(height);
        
        if max_dim > self.max_dimension {
            let scale = self.max_dimension as f32 / max_dim as f32;
            let new_width = (width as f32 * scale) as u32;
            let new_height = (height as f32 * scale) as u32;
            
            tracing::info!("Resizing {}x{} ‚Üí {}x{}", width, height, new_width, new_height);
            img.resize(new_width, new_height, image::imageops::FilterType::Lanczos3)
        } else {
            img
        }
    }
    
    /// Analyze image with Vision LLM (GPT-4o)
    async fn analyze_with_llm(&self, base64_image: &str) -> Result<ImageAnalysis> {
        let prompt = format!(
            r#"Analyze this image and extract structured information.

Return ONLY valid JSON (no markdown, no explanation):
{{
  "description": "Natural language description (1-2 sentences)",
  "image_type": "selfie/screenshot/meme/document/photo/other",
  "scene": "indoor/outdoor/unknown",
  "objects": ["object1", "object2", "object3"],
  "people_count": 0,
  "emotions": ["emotion1", "emotion2"],
  "ocr_text": "Text found in image or null",
  "contains_pii": false,
  "pii_types": [],
  "context_tags": ["tag1", "tag2", "tag3"],
  "is_meme": false,
  "quality": "high/medium/low"
}}

Guidelines:
- Be concise but descriptive
- Detect PII (faces, IDs, credit cards, addresses)
- Extract ALL text visible (OCR)
- Classify image type accurately
- Infer emotional context from facial expressions, colors, composition
- Context tags: 3-5 main themes/topics
- Quality based on resolution, focus, lighting
"#
        );
        
        // Call Vision API
        let response = self.llm.chat_with_vision(&prompt, base64_image).await?;
        
        // Parse JSON
        let extracted: ExtractedImageData = serde_json::from_str(&response)
            .map_err(|e| ImageAnalysisError::ParseError {
                details: e.to_string(),
            })?;
        
        // Build analysis
        Ok(ImageAnalysis {
            description: extracted.description,
            image_type: extracted.image_type,
            scene: extracted.scene,
            objects: extracted.objects,
            people_count: extracted.people_count,
            emotions: extracted.emotions,
            ocr_text: extracted.ocr_text,
            contains_pii: extracted.contains_pii,
            pii_types: extracted.pii_types,
            context_tags: extracted.context_tags,
            is_meme: extracted.is_meme,
            quality: extracted.quality,
            analyzed_at: chrono::Utc::now(),
            model: "gpt-4o-vision".to_string(),
            cost: 0.005,
        })
    }
    
    /// Generate cache key (path hash + file hash)
    async fn cache_key(&self, path: &Path) -> Result<String> {
        use sha2::{Sha256, Digest};
        use tokio::fs;
        
        // Read file for hash
        let bytes = fs::read(path).await?;
        
        // Hash: SHA256(path + content)
        let mut hasher = Sha256::new();
        hasher.update(path.to_string_lossy().as_bytes());
        hasher.update(&bytes);
        
        Ok(format!("{:x}", hasher.finalize()))
    }
}

/// Extracted data from Vision LLM
#[derive(Debug, Deserialize)]
struct ExtractedImageData {
    description: String,
    image_type: String,
    scene: String,
    objects: Vec<String>,
    people_count: u32,
    emotions: Vec<String>,
    ocr_text: Option<String>,
    contains_pii: bool,
    pii_types: Vec<String>,
    context_tags: Vec<String>,
    is_meme: bool,
    quality: String,
}

/// Final image analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ImageAnalysis {
    pub description: String,
    pub image_type: String,
    pub scene: String,
    pub objects: Vec<String>,
    pub people_count: u32,
    pub emotions: Vec<String>,
    pub ocr_text: Option<String>,
    pub contains_pii: bool,
    pub pii_types: Vec<String>,
    pub context_tags: Vec<String>,
    pub is_meme: bool,
    pub quality: String,
    pub analyzed_at: chrono::DateTime<chrono::Utc>,
    pub model: String,
    pub cost: f64,
}

impl ImageAnalysis {
    /// Placeholder when budget exceeded
    fn budget_exceeded() -> Self {
        Self {
            description: "Analysis skipped (budget exceeded)".to_string(),
            image_type: "unknown".to_string(),
            scene: "unknown".to_string(),
            objects: vec![],
            people_count: 0,
            emotions: vec![],
            ocr_text: None,
            contains_pii: false,
            pii_types: vec![],
            context_tags: vec![],
            is_meme: false,
            quality: "unknown".to_string(),
            analyzed_at: chrono::Utc::now(),
            model: "none".to_string(),
            cost: 0.0,
        }
    }
}
```

### 8.2. Budget Tracker

```rust
// src/data_import/budget_tracker.rs

use std::sync::Arc;
use tokio::sync::Mutex;
use chrono::{Utc, Timelike};

/// Track daily API costs
pub struct BudgetTracker {
    /// Daily limit (USD)
    limit: f64,
    
    /// Current day's spending
    state: Arc<Mutex<BudgetState>>,
}

struct BudgetState {
    date: chrono::NaiveDate,
    spent: f64,
}

impl BudgetTracker {
    pub fn new(daily_limit: f64) -> Self {
        Self {
            limit: daily_limit,
            state: Arc::new(Mutex::new(BudgetState {
                date: Utc::now().date_naive(),
                spent: 0.0,
            })),
        }
    }
    
    /// Record spending
    pub async fn record(&self, cost: f64) -> Result<()> {
        let mut state = self.state.lock().await;
        let today = Utc::now().date_naive();
        
        // Reset if new day
        if state.date != today {
            state.date = today;
            state.spent = 0.0;
        }
        
        state.spent += cost;
        
        tracing::info!("Budget: ${:.4} / ${:.2} today", state.spent, self.limit);
        
        Ok(())
    }
    
    /// Check if budget exceeded
    pub async fn is_exceeded(&self) -> Result<bool> {
        let state = self.state.lock().await;
        let today = Utc::now().date_naive();
        
        if state.date != today {
            // New day, budget OK
            Ok(false)
        } else {
            Ok(state.spent >= self.limit)
        }
    }
}
```

### 8.3. Integration con HyperlinkExtractor

```rust
// src/data_import/extraction.rs

impl HyperlinkExtractor {
    pub async fn extract(&self, digested: &DigestedData) -> Result<ExtractedNutrients> {
        // ... existing code ...
        
        // Extract attachments (images)
        let attachments = self.extract_attachments(digested)?;
        
        for attachment in attachments {
            if self.is_image(&attachment) {
                // Analyze image
                let analysis = self.image_analyzer.analyze(&attachment.path).await?;
                
                // Convert to nutrients
                let nutrients = self.image_to_nutrients(analysis, &attachment);
                all_nutrients.extend(nutrients);
            }
        }
        
        Ok(all_nutrients)
    }
    
    /// Check if attachment is image
    fn is_image(&self, attachment: &Attachment) -> bool {
        matches!(
            attachment.mime_type.as_str(),
            "image/jpeg" | "image/png" | "image/webp" | "image/gif"
        )
    }
    
    /// Convert ImageAnalysis to nutrients
    fn image_to_nutrients(&self, analysis: ImageAnalysis, attachment: &Attachment) -> Vec<Nutrient> {
        let mut nutrients = Vec::new();
        
        // Main description nutrient
        nutrients.push(Nutrient {
            dimension: "VISUAL".to_string(),
            key: format!("image_description_{}", attachment.filename),
            value: analysis.description.clone(),
            confidence: 0.85,
            source: "vision_llm".to_string(),
            context: Some(format!("Image: {}", attachment.filename)),
        });
        
        // Emotional nutrients
        for emotion in &analysis.emotions {
            nutrients.push(Nutrient {
                dimension: "EMOTIONAL".to_string(),
                key: format!("image_emotion_{}", emotion),
                value: emotion.clone(),
                confidence: 0.75,
                source: "vision_llm".to_string(),
                context: Some(format!("From image: {}", attachment.filename)),
            });
        }
        
        // Context tags
        for tag in &analysis.context_tags {
            nutrients.push(Nutrient {
                dimension: "INTEREST".to_string(),
                key: format!("image_topic_{}", tag),
                value: tag.clone(),
                confidence: 0.70,
                source: "vision_llm".to_string(),
                context: Some(format!("Visual topic from: {}", attachment.filename)),
            });
        }
        
        // OCR text (if any)
        if let Some(ocr_text) = &analysis.ocr_text {
            nutrients.push(Nutrient {
                dimension: "TEXTUAL".to_string(),
                key: format!("image_ocr_{}", attachment.filename),
                value: ocr_text.clone(),
                confidence: 0.80,
                source: "vision_llm_ocr".to_string(),
                context: Some(format!("Text extracted from: {}", attachment.filename)),
            });
        }
        
        // PII warning (if detected)
        if analysis.contains_pii {
            nutrients.push(Nutrient {
                dimension: "PRIVACY".to_string(),
                key: format!("image_pii_warning_{}", attachment.filename),
                value: format!("Contains: {:?}", analysis.pii_types),
                confidence: 0.95,
                source: "vision_llm".to_string(),
                context: Some(format!("Privacy alert: {}", attachment.filename)),
            });
        }
        
        nutrients
    }
}
```

---

## 9. EST√ÅNDARES

### 9.1. Error Handling

```rust
#[derive(Debug, thiserror::Error)]
pub enum ImageAnalysisError {
    #[error("Failed to load image {path:?}: {reason}")]
    LoadFailed {
        path: PathBuf,
        reason: String,
    },
    
    #[error("Failed to encode image: {reason}")]
    EncodeFailed {
        reason: String,
    },
    
    #[error("Vision LLM API error: {details}")]
    VisionAPIError {
        details: String,
    },
    
    #[error("Failed to parse LLM response: {details}")]
    ParseError {
        details: String,
    },
    
    #[error("Daily budget exceeded (${limit})")]
    BudgetExceeded {
        limit: f64,
    },
}
```

### 9.2. Logging

```rust
// Startup check
tracing::info!("ImageAnalyzer initialized (GPT-4o Vision, $5/day budget)");

// Per-image logging
tracing::info!("Analyzing image: {:?}", path);
tracing::debug!("Image size: {}x{}, resized: {}", width, height, resized);
tracing::info!("Cache {}: {:?}", if hit { "HIT" } else { "MISS" }, path);
tracing::info!("Budget: ${:.4} / $5.00 today", spent);

// Error logging
tracing::error!("Vision API failed: {}", error);
tracing::warn!("Budget exceeded, skipping image analysis");
```

### 9.3. Privacy Guidelines

```yaml
PII Detection:
  - Faces: ALWAYS detect (contains_pii: true, pii_types: ["faces"])
  - IDs/Documents: Check for text patterns (passports, licenses)
  - Credit Cards: Detect 16-digit numbers + CVV patterns
  - Addresses: Full addresses in OCR text
  - Phone Numbers: Detect phone number patterns

User Control:
  - Setting: analyze_images (default: true)
  - Setting: blur_faces_in_images (default: false, v2.0)
  - Setting: skip_pii_images (default: false)

Storage:
  - Original images: NEVER stored in Bit√°cora DB
  - Analysis results: Stored (description, tags, emotions)
  - Base64 images: NEVER cached (only analysis results)
```

---

## 10. CHECKLIST

### Pre-Implementation

- [ ] Read GUIA.md (Theremin philosophy)
- [ ] Read METOD_DOCS.md (methodology)
- [ ] Confirm GPT-4o Vision API access
- [ ] Test Vision API with sample image
- [ ] Estimate cost for Paula chat (406 photos x $0.005)

### Phase 1 (v1.0) - Basic Vision Analysis

- [ ] Create `src/data_import/content_extractors/image.rs`
- [ ] Implement `ImageAnalyzer` struct
- [ ] Add `image` + `base64` dependencies
- [ ] Implement `load_and_encode()` (resize + base64)
- [ ] Implement `analyze_with_llm()` (Vision prompt)
- [ ] Implement cache layer (30 d√≠as)
- [ ] Implement `BudgetTracker` ($5/day limit)
- [ ] Add error handling (`ImageAnalysisError`)
- [ ] Add logging (tracing)
- [ ] Create `examples/test_image_analyzer.rs`
- [ ] Test con 10 fotos de Paula chat
- [ ] Validate descriptions accuracy (>90%)
- [ ] Validate PII detection (>95%)
- [ ] Benchmark cost ($0.005/image expected)
- [ ] Integration con `HyperlinkExtractor`
- [ ] Test end-to-end (WhatsApp import ‚Üí nutrients)
- [ ] Update `CHECKLIST_V2.md` (Task 7.x.3.9: ImageAnalyzer)
- [ ] Git commit: "feat(extraction): ImageAnalyzer v1.0 - Vision LLM delegation"

### Phase 2 (v1.1) - Context Correlation

- [ ] Correlate emotions con `EmotionalSpace`
- [ ] Detect temporal patterns (vacation photos on weekends)
- [ ] Relationship detection (photos with whom)
- [ ] Topic extraction (beach ‚Üí travel, book ‚Üí literature)
- [ ] Test correlation accuracy
- [ ] Update documentation

### Phase 3 (v2.0) - Privacy Enhancement

- [ ] Add `opencv` dependency (face detection)
- [ ] Implement face blurring
- [ ] Implement OCR + text masking
- [ ] Store blurred versions
- [ ] User setting: `blur_faces_in_images`
- [ ] Test privacy features

---

## üìö REFERENCIAS

### Documentos Relacionados

- `ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md` - Parent document
- `ROADMAP_V2/02_COMPONENTES/18.1_hyperlink-content-extraction.md` - Multimedia extraction
- `ROADMAP_V2/02_COMPONENTES/18.3_voice-to-text-transcription.md` - Audio transcription
- `ROADMAP_V2/CHECKLIST_V2.md` - Task 7.x.3.9: ImageAnalyzer

### External Resources

- [GPT-4o Vision API Docs](https://platform.openai.com/docs/guides/vision)
- [Claude Sonnet Vision](https://docs.anthropic.com/claude/docs/vision)
- [Gemini Vision](https://ai.google.dev/tutorials/vision_quickstart)
- [Image crate docs](https://docs.rs/image/)

---

## üéØ DECISI√ìN ESTRAT√âGICA

```yaml
v1.0: Vision LLM delegation (GPT-4o)
  Why:
    - Context-aware (entiende SIGNIFICADO, no solo pixels)
    - Cost-effective ($2-5/user one-time, $75-150/month scaled)
    - Zero maintenance (no model training, no updates)
    - Privacy-aware (detecta PII)
    - NO bloquea MVP
  
  Trade-offs:
    - Latency: ~4s (vs <100ms Computer Vision)
    - Privacy: images go to OpenAI (mitigable con cache)
    - Cost: ~$0.005/image (vs $0 local)
  
  Conclusion:
    For v1.0 (WhatsApp imports, NOT real-time), Vision LLMs are IDEAL.
    Context understanding >> speed.
    Developer time saved >> API costs.
```

**Philosophy:** "Delegar a quien lo hace mejor. Bit√°cora es memoria personal, NO Computer Vision research."
