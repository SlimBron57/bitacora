```yaml
# === DATOS DE AUDITOR√çA ===
Archivo: ROADMAP_V2/02_COMPONENTES/18.1_hyperlink-content-extraction.md
Versi√≥n: 1.0
Fecha Creaci√≥n: 2025-11-29
√öltima Actualizaci√≥n: 2025-11-29 20:45:00
Autor: Sistema Bit√°cora + Eduardo
Prop√≥sito: Especificaci√≥n t√©cnica de Content Extraction para HyperlinkExtractor
Estado: üéØ ACTIVO - Arquitectura t√©cnica multimedia extraction
Inspiraci√≥n: 
  - yt-dlp: Extracci√≥n robusta YouTube sin API quotas
  - Readability.js: Extracci√≥n inteligente de contenido web
  - Whisper API: State-of-the-art speech-to-text
  - BitTorrent: Principio de cache distribuido
  - Archive.org: Filosof√≠a de preservaci√≥n de contenido
Filosof√≠a: >
  "No todos los links son iguales. Un video de 30 min tiene m√°s densidad 
  que un meme. Una playlist de Spotify revela gustos, no solo canciones.
  Un art√≠culo acad√©mico es conocimiento, no solo texto. Bit√°cora entiende 
  esta diferencia porque respeta la NATURALEZA del contenido, no solo su URL."
Cambios v1.0 (2025-11-29 20:45:00):
  - üìö Documento inicial creado siguiendo METOD_DOCS.md
  - üèóÔ∏è Architecture: 5 extractors especializados (YouTube, Spotify, Webpage, Audio, Video)
  - üíæ Cache layer: SQLite 7 d√≠as TTL
  - üí∞ Budget tracking: Whisper API $5/day max
  - ‚ö° Rate limiting: Domain-specific throttling
  - üß™ Testing strategy: 4 test cases + benchmarks
  - üìã Implementation roadmap: 6 phases (Infrastructure ‚Üí Video)
  - üéØ Decisiones Arquitect√≥nicas aplicadas: DA-XXX (Local-First), DA-XXX (Privacy), DA-XXX (Cost-Conscious)
Punto de Entrada:
  - üÜï Nuevo al documento? ‚Üí Empieza con SECCI√ìN 2 (Prop√≥sito del M√≥dulo)
  - üî® Vas a implementar? ‚Üí Ve directo a SECCI√ìN 6 (Plan de Acci√≥n) ‚Üí Phase 1
  - üß™ Vas a testear? ‚Üí SECCI√ìN 7 (Validaci√≥n) tiene test cases completos
  - üìä Necesitas contexto? ‚Üí SECCI√ìN 1 (Inventario F√≠sico) + SECCI√ìN 4 (Estado Actual)
  - üêõ Debugging? ‚Üí SECCI√ìN 5 (Problemas Detectados) lista issues conocidos
  - üìö Referencia r√°pida? ‚Üí SECCI√ìN 9 (Est√°ndares) + SECCI√ìN 10 (Checklist)
Relacionado Con:
  - ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md (documento padre)
  - ROADMAP_V2/02_COMPONENTES/01_sensory-engine.md (Whisper API integration)
  - ROADMAP_V2/FUSION_BAYESIANA/00_INDICE.md (DA-XXX: Local-First, Privacy, Cost-Conscious)
  - src/data_import/extraction.rs (HyperlinkExtractor implementation)
  - src/multi_agent/llm_client.rs (HTTP client reference pattern)
Categor√≠a: TECHNICAL_ARCHITECTURE
Prioridad: ALTA - Bloqueador para HyperlinkExtractor v1.1+
Metodolog√≠a: METOD_DOCS.md (10-step systematic documentation)
# === FIN DATOS DE AUDITOR√çA ===
```

# üîå Hyperlink Content Extraction - Multimedia & Web Scraping

> **Component:** HyperlinkContentExtractor (Subsistema de HyperlinkExtractor)  
> **Purpose:** Extraer contenido enriquecido de URLs (audio, video, websites) para an√°lisis profundo
>
> **Filosof√≠a:** *"Una URL es una promesa. El contenido es la verdad. No juzgues un libro por su link."*

---

## üé≠ PRELUDIO: POR QU√â ESTE DOCUMENTO EXISTE

### El Problema Humano Detr√°s del C√≥digo

Imagina esto:

Compartiste 50 links con un amigo el √∫ltimo a√±o:
- 20 videos de YouTube (tutoriales, m√∫sica, charlas)
- 15 art√≠culos de blogs t√©cnicos
- 10 playlists de Spotify
- 5 repositorios de GitHub

**Pregunta simple:** ¬øQu√© dicen esos links sobre ti?

**Respuesta superficial:** "Le gustan los tutoriales y la m√∫sica electr√≥nica"

**Respuesta profunda (con content extraction):**
- Videos: 80% educativos (ML, Rust, arquitectura), 20% entretenimiento
- Duraci√≥n promedio: 25 minutos (consumo profundo, no scroll)
- Transcripts muestran keywords: "distributed systems", "async", "performance"
- Spotify: 70% focus music (lo-fi, ambient), 30% rock progresivo
- Tempo promedio: 110 BPM (m√∫sica para trabajar, no bailar)
- GitHub: Contribuciones a proyectos Rust, temas: CLI tools, parsers
- Art√≠culos: 90% long-form (>2000 palabras), lenguaje t√©cnico avanzado

**Conclusi√≥n:** Esta persona es un **ingeniero senior que optimiza para deep work**, valora aprendizaje continuo, y consume contenido deliberadamente (curator, no consumer).

**Eso NO lo puedes inferir solo de URLs.** Necesitas el **contenido**.

### El Desaf√≠o T√©cnico

Extraer contenido de URLs es **f√°cil en teor√≠a, complejo en pr√°ctica:**

```
URL ‚Üí Contenido

Teor√≠a:  fetch(url) ‚Üí text
Pr√°ctica: 
  - YouTube requiere yt-dlp (no API simple)
  - Spotify requiere OAuth (tokens, refresh)
  - Websites tienen HTML diferente (no hay est√°ndar)
  - Audio requiere transcripci√≥n ($$$)
  - Video requiere ffmpeg (pesado)
  - Todo requiere cache (rate limits)
  - Todo requiere error handling (links mueren)
```

Este documento resuelve esa complejidad **sistem√°ticamente**.

---

## üìã TABLA DE CONTENIDOS

1. [Inventario F√≠sico](#inventario-f√≠sico)
2. [Prop√≥sito del M√≥dulo](#prop√≥sito-del-m√≥dulo)
3. [Flujo L√≥gico Ideal](#flujo-l√≥gico-ideal)
4. [Mapeo Actual vs Ideal](#mapeo-actual-vs-ideal)
5. [Detecci√≥n de Problemas](#detecci√≥n-de-problemas)
6. [Plan de Acci√≥n](#plan-de-acci√≥n)
7. [Validaci√≥n Post-Cambio](#validaci√≥n-post-cambio)
8. [Herramientas Autom√°ticas](#herramientas-autom√°ticas)
9. [Est√°ndares Globales](#est√°ndares-globales)
10. [Checklist de Ejecuci√≥n](#checklist-de-ejecuci√≥n)

---

## 1Ô∏è‚É£ INVENTARIO F√çSICO

### 1.1 Archivos Existentes Relacionados

```bash
# B√∫squeda de archivos relacionados (2025-11-29)
$ find . -name "*hyperlink*" -o -name "*content*extract*" -o -name "*llm_client*"
```

**Resultado:**
```yaml
Archivos Existentes:
  - ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md (1,000+ l√≠neas) ‚úÖ
  - ROADMAP_V2/02_COMPONENTES/01_sensory-engine.md (Whisper API, 1,200+ l√≠neas) ‚úÖ
  - src/multi_agent/llm_client.rs (HTTP client con reqwest) ‚úÖ
  - src/data_import/extraction.rs (~2,082 l√≠neas, 7 extractors) ‚úÖ

Archivos NO Existentes (a crear):
  - src/data_import/content_extractors/ (directorio nuevo)
  - src/data_import/content_extractors/mod.rs
  - src/data_import/content_extractors/youtube.rs
  - src/data_import/content_extractors/spotify.rs
  - src/data_import/content_extractors/webpage.rs
  - src/data_import/content_extractors/audio.rs
  - src/data_import/content_extractors/video.rs

Duplicados: Ninguno
Hu√©rfanos: Ninguno (todo referenciado en CHECKLIST)
```

### 1.2 An√°lisis de Dependencias

```toml
# Cargo.toml - Dependencias actuales
[dependencies]
reqwest = { version = "0.11", features = ["json"] }  # ‚úÖ HTTP client
tokio = { version = "1", features = ["full"] }        # ‚úÖ Async runtime
serde = { version = "1.0", features = ["derive"] }    # ‚úÖ Serialization

# Dependencias NECESARIAS (a agregar)
# youtube_dl = "0.8"                    # ‚ùå NO - usar yt-dlp via CLI
# yt-dlp (via std::process::Command)    # ‚úÖ S√ç - mejor mantenimiento
# scraper = "0.17"                      # ‚úÖ S√ç - HTML parsing
# select = "0.6"                        # Alternative HTML parsing
# reqwest-middleware = "0.2"            # Rate limiting, retry logic
```

---

## 2Ô∏è‚É£ PROP√ìSITO DEL M√ìDULO

### 2.1 Problema que Resuelve

**Contexto:**
- HyperlinkExtractor v1.0 solo extrae URLs + metadata b√°sica (platform, intent)
- **NO extrae contenido profundo** de las URLs (t√≠tulos, descripciones, transcripciones)
- Usuarios comparten videos de YouTube ‚Üí queremos saber **de qu√© hablan**
- Usuarios comparten playlists de Spotify ‚Üí queremos saber **qu√© g√©neros escuchan**
- Usuarios comparten art√≠culos ‚Üí queremos **texto completo** para an√°lisis

**Limitaci√≥n Actual:**
```rust
// HyperlinkExtractor v1.0 (b√°sico)
ClassifiedLink {
    url: "https://youtube.com/watch?v=abc123",
    platform: Platform::YouTube,
    intent: LinkIntent::Recommendation,
    // ‚ùå NO HAY CONTENIDO PROFUNDO
}
```

**Objetivo v1.1+:**
```rust
// HyperlinkExtractor v1.1 (enriquecido)
ClassifiedLink {
    url: "https://youtube.com/watch?v=abc123",
    platform: Platform::YouTube,
    intent: LinkIntent::Recommendation,
    // ‚úÖ CONTENIDO ENRIQUECIDO
    metadata: LinkMetadata {
        title: "How to Build LLM Apps",
        description: "Complete guide to...",
        duration: Some(Duration::from_secs(1820)),  // 30:20
        transcript: Some("Welcome to this tutorial..."),
        author: "OpenAI",
        tags: vec!["AI", "LLM", "Tutorial"],
    },
}
```

### 2.2 Principios de Dise√±o

**Antes de definir responsabilidades, definimos principios:**

#### Principio 1: Respeto por la Naturaleza del Contenido

```
Un video NO es un art√≠culo.
Un art√≠culo NO es una canci√≥n.
Una canci√≥n NO es un video.

Cada medio tiene:
  - Densidad informativa diferente
  - Velocidad de consumo diferente
  - Intenci√≥n comunicativa diferente

YouTube 30min = 7,500 palabras transcript
Art√≠culo 30min = 2,000 palabras lectura
Spotify 30min = 8-10 canciones, 0 palabras

Content extraction debe RESPETAR estas diferencias.
```

#### Principio 2: Cache Agresivo, Fetch Conservador

```
Links no cambian frecuentemente.
APIs tienen rate limits.
Transcripci√≥n cuesta dinero.

Soluci√≥n:
  - Cache: 7 d√≠as (balance entre freshness y cost)
  - Fetch: Solo cuando necesario
  - Budget: Hard limit $5/day Whisper
```

#### Principio 3: Graceful Degradation

```
NO todos los links son extra√≠bles.
Videos pueden ser privados.
Websites pueden bloquear scrapers.
APIs pueden estar down.

Regla:
  Si 1 link falla ‚Üí continuar con otros 99
  Reportar error, no crash
```

#### Principio 4: Privacy-First

```
Transcripts pueden contener PII.
URLs pueden tener tokens de sesi√≥n.
Cache puede ser sensible.

Garant√≠as:
  - No guardar tokens en cache
  - Detectar PII en transcripts (nombres, emails)
  - User consent para transcripci√≥n
  - Local-first (DA-XXX)
```

### 2.2 Responsabilidades del M√≥dulo

```yaml
Content Extraction Responsibilities:

1. YouTube Videos:
   - Metadata: title, description, author, duration, view count
   - Thumbnail URL
   - Transcript (captions) si disponible
   - Tags/categories

2. Spotify Tracks/Playlists:
   - Metadata: track name, artist, album, duration
   - Genre classification
   - Audio features (tempo, energy, danceability) - Spotify API

3. Generic Websites:
   - Open Graph metadata (og:title, og:description, og:image)
   - Structured data (JSON-LD, Schema.org)
   - Text content (clean, sin HTML)
   - Language detection

4. Audio Files (direct links .mp3, .wav):
   - Transcription via Whisper API
   - Duration detection
   - Audio quality metrics

5. Video Files (direct links .mp4, .webm):
   - Metadata extraction (duration, resolution, codec)
   - Frame extraction (thumbnail generation)
   - Audio transcription (extract audio ‚Üí Whisper)

Rate Limiting & Caching:
   - Respetar robots.txt
   - Cache agresivo (TTL 7 d√≠as)
   - Retry logic (exponential backoff)
   - Concurrent limit (max 5 requests paralelos)
```

---

## 3Ô∏è‚É£ FLUJO L√ìGICO IDEAL

### 3.1 Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              HyperlinkExtractor (Parent)                    ‚îÇ
‚îÇ  Input: DigestedData (messages with URLs)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Phase 1: URL Extraction & Classification           ‚îÇ
‚îÇ  - Extract URLs (regex)                                     ‚îÇ
‚îÇ  - Classify platform (YouTube, Spotify, Generic)            ‚îÇ
‚îÇ  - Expand short URLs (bit.ly ‚Üí real URL)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Phase 2: Content Extraction (THIS MODULE)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ContentExtractorRouter                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Match platform ‚Üí specific extractor               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Parallel extraction (tokio::spawn per URL)        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                            ‚îÇ                                ‚îÇ
‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ        ‚ñº                   ‚ñº                   ‚ñº           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ YouTube  ‚îÇ      ‚îÇ Spotify  ‚îÇ      ‚îÇ Webpage  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇExtractor ‚îÇ      ‚îÇExtractor ‚îÇ      ‚îÇExtractor ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ        ‚ñº                   ‚ñº                   ‚ñº           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ yt-dlp   ‚îÇ      ‚îÇ Spotify  ‚îÇ      ‚îÇ scraper  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ (CLI)    ‚îÇ      ‚îÇ   API    ‚îÇ      ‚îÇ (Rust)   ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ContentCache (SQLite)                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Key: URL hash (SHA-256)                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Value: ExtractedContent + timestamp               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - TTL: 7 days                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Phase 3: Content Analysis & Intelligence           ‚îÇ
‚îÇ  - Intent inference (updated with content context)          ‚îÇ
‚îÇ  - Topic extraction (from titles, transcripts)              ‚îÇ
‚îÇ  - Efficiency scoring (educational vs entertainment)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Flujo Detallado por Plataforma

#### 3.2.1 YouTube Extractor: La Filosof√≠a yt-dlp

> **Reflexi√≥n Previa:** *"YouTube tiene API oficial, ¬øpor qu√© usar yt-dlp?"*
>
> **Respuesta:** Porque YouTube API tiene:
> - Quotas estrictas (10,000 units/day por proyecto)
> - Requiere API key (setup friction)
> - Solo metadata b√°sica (NO subtitles autom√°ticos)
> - Rate limiting agresivo
>
> yt-dlp es:
> - ‚úÖ Sin quotas (scraping inteligente)
> - ‚úÖ Sin API keys (friction zero)
> - ‚úÖ Subtitles completos (auto-generated + manuales)
> - ‚úÖ Metadata exhaustiva (formato completo)
> - ‚úÖ Mantenido activamente (fork de youtube-dl)
> - ‚úÖ Respeta robots.txt (√©tico)

##### ¬øQu√© es yt-dlp?

**Historia:**
```
2006: youtube-dl nace (Python, CLI simple)
  ‚Üì
2010-2020: Domina descarga de videos
  ‚Üì
2021: DMCA takedown controversy (GitHub)
  ‚Üì
2021: Fork ‚Üí yt-dlp (community-driven)
  ‚Üì
2025: yt-dlp es EL est√°ndar (600+ sites)
```

**Arquitectura yt-dlp:**
```
yt-dlp CLI
  ‚îú‚îÄ Extractors (600+ sitios)
  ‚îÇ   ‚îú‚îÄ YouTube (m√°s completo)
  ‚îÇ   ‚îú‚îÄ Vimeo
  ‚îÇ   ‚îú‚îÄ Twitch
  ‚îÇ   ‚îî‚îÄ ...
  ‚îú‚îÄ Downloaders (HTTP, HLS, DASH)
  ‚îú‚îÄ Post-processors (ffmpeg integration)
  ‚îî‚îÄ Output formatters (JSON, text, etc.)
```

**Por qu√© es perfecto para Bit√°cora:**
1. **No inventamos la rueda** - yt-dlp ya resuelve scraping complejo
2. **Metadata exhaustiva** - JSON con 50+ campos
3. **Subtitles autom√°ticos** - Crucial para transcripts
4. **Zero config** - `pip install yt-dlp`, funciona
5. **√âtico** - No viola ToS, usa endpoints p√∫blicos

##### Flujo de Extracci√≥n

```rust
// Flow: YouTube URL ‚Üí Content Extraction
async fn extract_youtube_content(url: &str) -> Result<YouTubeContent> {
    // Step 1: Check cache
    if let Some(cached) = CACHE.get(url).await? {
        if !cached.is_expired() {
            return Ok(cached.content);
        }
    }
    
    // Step 2: Extract video ID
    let video_id = extract_youtube_id(url)?;
    
    // Step 3: Call yt-dlp for metadata + transcript
    // 
    // Filosof√≠a: yt-dlp hace el heavy lifting.
    // Nosotros solo orquestamos y parseamos.
    let metadata = tokio::process::Command::new("yt-dlp")
        .args(&[
            "--dump-json",           // JSON output (50+ campos)
            "--write-auto-sub",      // Download auto-generated captions
            "--sub-lang", "en,es",   // Priorizar ingl√©s/espa√±ol
            "--skip-download",       // Solo metadata, NO video
            "--no-warnings",         // Silenciar warnings
            &format!("https://www.youtube.com/watch?v={}", video_id),
        ])
        .output()
        .await?;
    
    // Step 4: Parse JSON
    // yt-dlp retorna JSON exhaustivo:
    // {
    //   "id": "dQw4w9WgXcQ",
    //   "title": "Rick Astley - Never Gonna Give You Up",
    //   "description": "...",
    //   "duration": 212,
    //   "view_count": 1000000000,
    //   "like_count": 12000000,
    //   "upload_date": "20091025",
    //   "uploader": "Rick Astley",
    //   "tags": ["80s", "music", ...],
    //   "categories": ["Music"],
    //   "automatic_captions": {"en": [{"url": "..."}]},
    //   "subtitles": {...},
    //   ... (40+ campos m√°s)
    // }
    let json: YouTubeMetadata = serde_json::from_slice(&metadata.stdout)?;
    
    // Step 5: Extract transcript (if available)
    // Prioridad: manual subtitles > auto-generated
    let transcript = if let Some(subtitles) = &json.subtitles {
        Some(parse_vtt_subtitles(subtitles)?)
    } else if let Some(auto_captions) = &json.automatic_captions {
        Some(parse_vtt_subtitles(auto_captions)?)
    } else {
        None
    };
    
    // Step 6: Build content
    let content = YouTubeContent {
        video_id,
        title: json.title,
        description: json.description,
        author: json.uploader,
        duration: Duration::from_secs(json.duration),
        view_count: json.view_count,
        like_count: json.like_count,
        upload_date: json.upload_date,
        tags: json.tags,
        categories: json.categories,
        transcript,
        thumbnail_url: json.thumbnail,
    };
    
    // Step 7: Cache result (7 d√≠as)
    CACHE.set(url, &content, Duration::from_days(7)).await?;
    
    Ok(content)
}
```

##### Parsing VTT Subtitles

```rust
/// Parse WebVTT (Video Text Tracks) format to plain text
/// 
/// VTT Format:
/// ```
/// WEBVTT
/// 
/// 00:00:00.000 --> 00:00:03.000
/// Never gonna give you up
/// 
/// 00:00:03.000 --> 00:00:06.000
/// Never gonna let you down
/// ```
fn parse_vtt_subtitles(vtt_content: &str) -> Result<String> {
    let lines: Vec<&str> = vtt_content
        .lines()
        .filter(|line| {
            // Skip timestamps (contienen "-->")
            !line.contains("-->") &&
            // Skip headers (WEBVTT, etc.)
            !line.starts_with("WEBVTT") &&
            // Skip empty lines
            !line.trim().is_empty()
        })
        .collect();
    
    Ok(lines.join(" "))
}
```

##### ¬øFunciona con YouTube Shorts?

**‚úÖ S√ç - yt-dlp maneja Shorts nativamente**

```yaml
YouTube Shorts:
  URL Format: https://youtube.com/shorts/{video_id}
  
C√≥mo yt-dlp lo maneja:
  - Detecta autom√°ticamente formato Shorts
  - Extrae metadata id√©ntica a videos normales
  - Subtitles autom√°ticos (si disponibles)
  - Duraci√≥n t√≠pica: 15-60 segundos
  
Diferencias con videos normales:
  - Campo "format": "shorts" (metadata)
  - Duraci√≥n m√°s corta
  - Aspect ratio: 9:16 (vertical)
  - NO tiene chapters/timestamps
  
Ejemplo Real:
  Input: https://youtube.com/shorts/abc123
  Output (yt-dlp):
    {
      "id": "abc123",
      "title": "Cool Short Video",
      "duration": 45,
      "format": "shorts",
      "aspect_ratio": 0.5625,  # 9:16
      "description": "...",
      "automatic_captions": {...}
    }
```

**Bit√°cora NO necesita c√≥digo especial** - yt-dlp abstrae la diferencia.

##### Integraci√≥n en Rust

**Arquitectura de Integraci√≥n:**

```rust
// src/data_import/content_extractors/youtube.rs

use tokio::process::Command;
use serde::{Deserialize, Serialize};
use std::time::Duration;

/// YouTube content extractor usando yt-dlp CLI
pub struct YouTubeExtractor {
    /// Cache manager
    cache: ContentCache,
    
    /// yt-dlp binary path (default: "yt-dlp")
    ytdlp_path: String,
    
    /// Max concurrent extractions
    semaphore: tokio::sync::Semaphore,
}

impl YouTubeExtractor {
    /// Crea nuevo extractor con defaults
    pub fn new(cache: ContentCache) -> Self {
        Self {
            cache,
            ytdlp_path: "yt-dlp".to_string(),
            semaphore: tokio::sync::Semaphore::new(5), // Max 5 concurrent
        }
    }
    
    /// Extrae contenido de URL de YouTube
    pub async fn extract(&self, url: &str) -> Result<YouTubeContent> {
        // Acquire semaphore (limit concurrent requests)
        let _permit = self.semaphore.acquire().await?;
        
        // Check cache first
        if let Some(cached) = self.cache.get(url).await? {
            return Ok(cached);
        }
        
        // Extract video ID
        let video_id = self.extract_video_id(url)?;
        
        // Call yt-dlp
        let output = Command::new(&self.ytdlp_path)
            .args(&[
                "--dump-json",
                "--write-auto-sub",
                "--sub-lang", "en,es",
                "--skip-download",
                "--no-warnings",
                url,
            ])
            .output()
            .await
            .map_err(|e| ContentExtractionError::DependencyMissing {
                dependency: "yt-dlp".to_string(),
                hint: "Install: pip install yt-dlp".to_string(),
                details: e.to_string(),
            })?;
        
        // Check if command succeeded
        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);
            return Err(ContentExtractionError::ExtractionFailed {
                url: url.to_string(),
                reason: format!("yt-dlp failed: {}", stderr),
            });
        }
        
        // Parse JSON output
        let metadata: YtDlpOutput = serde_json::from_slice(&output.stdout)
            .map_err(|e| ContentExtractionError::ParseError {
                source: "yt-dlp JSON".to_string(),
                details: e.to_string(),
            })?;
        
        // Download subtitles if available
        let transcript = self.extract_transcript(&metadata).await?;
        
        // Build content
        let content = YouTubeContent {
            video_id,
            title: metadata.title,
            description: metadata.description,
            author: metadata.uploader,
            duration: Duration::from_secs(metadata.duration),
            view_count: metadata.view_count,
            like_count: metadata.like_count,
            upload_date: metadata.upload_date,
            tags: metadata.tags.unwrap_or_default(),
            categories: metadata.categories.unwrap_or_default(),
            transcript,
            thumbnail_url: metadata.thumbnail,
            is_short: metadata.is_short(),  // Detect Shorts
        };
        
        // Cache result
        self.cache.set(url, &content, Duration::from_days(7)).await?;
        
        Ok(content)
    }
    
    /// Extrae video ID de URL de YouTube
    /// 
    /// Soporta formatos:
    /// - https://youtube.com/watch?v=abc123
    /// - https://youtu.be/abc123
    /// - https://youtube.com/shorts/abc123
    /// - https://m.youtube.com/watch?v=abc123
    fn extract_video_id(&self, url: &str) -> Result<String> {
        use regex::Regex;
        
        let patterns = [
            r"(?:v=|/)([0-9A-Za-z_-]{11}).*",  // watch?v= o youtu.be/
            r"shorts/([0-9A-Za-z_-]{11})",     // shorts/
        ];
        
        for pattern in &patterns {
            let re = Regex::new(pattern).unwrap();
            if let Some(caps) = re.captures(url) {
                if let Some(id) = caps.get(1) {
                    return Ok(id.as_str().to_string());
                }
            }
        }
        
        Err(ContentExtractionError::InvalidUrl {
            url: url.to_string(),
            reason: "No valid YouTube video ID found".to_string(),
        })
    }
    
    /// Extrae transcript de subtitles
    async fn extract_transcript(&self, metadata: &YtDlpOutput) -> Result<Option<String>> {
        // Priority: manual subtitles > auto-generated
        let subtitle_url = metadata
            .subtitles
            .as_ref()
            .and_then(|s| s.get("en").or_else(|| s.get("es")))
            .or_else(|| {
                metadata
                    .automatic_captions
                    .as_ref()
                    .and_then(|a| a.get("en").or_else(|| a.get("es")))
            });
        
        if let Some(subtitle_info) = subtitle_url {
            // Download VTT file
            let vtt_url = &subtitle_info[0].url;
            let vtt_content = reqwest::get(vtt_url)
                .await?
                .text()
                .await?;
            
            // Parse VTT to plain text
            let transcript = self.parse_vtt(&vtt_content);
            Ok(Some(transcript))
        } else {
            Ok(None)
        }
    }
    
    /// Parse WebVTT format to plain text
    fn parse_vtt(&self, vtt: &str) -> String {
        vtt.lines()
            .filter(|line| {
                let line = line.trim();
                !line.is_empty() &&
                !line.contains("-->") &&
                !line.starts_with("WEBVTT") &&
                !line.starts_with("NOTE")
            })
            .collect::<Vec<_>>()
            .join(" ")
    }
}

/// Output de yt-dlp (subset de campos)
#[derive(Debug, Deserialize)]
struct YtDlpOutput {
    id: String,
    title: String,
    description: Option<String>,
    uploader: String,
    duration: u64,
    view_count: Option<u64>,
    like_count: Option<u64>,
    upload_date: Option<String>,
    tags: Option<Vec<String>>,
    categories: Option<Vec<String>>,
    thumbnail: String,
    
    /// Subtitles manuales
    subtitles: Option<std::collections::HashMap<String, Vec<SubtitleInfo>>>,
    
    /// Auto-generated captions
    automatic_captions: Option<std::collections::HashMap<String, Vec<SubtitleInfo>>>,
    
    /// Format info (para detectar Shorts)
    format: Option<String>,
    aspect_ratio: Option<f64>,
}

impl YtDlpOutput {
    /// Detecta si es YouTube Short
    fn is_short(&self) -> bool {
        // Short si:
        // 1. Format contiene "shorts"
        // 2. Aspect ratio ~0.56 (9:16)
        // 3. Duraci√≥n < 60s (heur√≠stica adicional)
        self.format.as_ref().map_or(false, |f| f.contains("short")) ||
        self.aspect_ratio.map_or(false, |ar| (ar - 0.5625).abs() < 0.1) ||
        (self.duration < 60 && self.aspect_ratio.is_some())
    }
}

#[derive(Debug, Deserialize)]
struct SubtitleInfo {
    url: String,
    ext: String,  // vtt, srt, etc.
}

/// Contenido extra√≠do de YouTube
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct YouTubeContent {
    pub video_id: String,
    pub title: String,
    pub description: Option<String>,
    pub author: String,
    pub duration: Duration,
    pub view_count: Option<u64>,
    pub like_count: Option<u64>,
    pub upload_date: Option<String>,
    pub tags: Vec<String>,
    pub categories: Vec<String>,
    pub transcript: Option<String>,
    pub thumbnail_url: String,
    pub is_short: bool,  // TRUE si es YouTube Short
}
```

**Dependency Checking al Startup:**

```rust
// src/data_import/content_extractors/mod.rs

/// Verifica que yt-dlp est√© instalado
pub async fn check_ytdlp_available() -> Result<bool> {
    match tokio::process::Command::new("yt-dlp")
        .arg("--version")
        .output()
        .await
    {
        Ok(output) => {
            let version = String::from_utf8_lossy(&output.stdout);
            tracing::info!("yt-dlp version: {}", version.trim());
            Ok(true)
        }
        Err(_) => {
            tracing::warn!("yt-dlp not found. YouTube extraction disabled.");
            tracing::warn!("Install: pip install yt-dlp");
            Ok(false)
        }
    }
}
```

**Uso desde HyperlinkExtractor:**

```rust
// src/data_import/extraction.rs

impl HyperlinkExtractor {
    pub async fn extract(&self, digested: &DigestedData) -> Result<ExtractedNutrients> {
        // Extract URLs
        let urls = self.extract_urls(digested)?;
        
        // Process each URL
        for url in urls {
            let platform = self.classify_platform(&url);
            
            match platform {
                Platform::YouTube => {
                    // Delegate to YouTubeExtractor
                    let content = self.youtube_extractor.extract(&url).await?;
                    
                    // Convert to nutrients
                    let nutrients = self.youtube_to_nutrients(content, &url);
                    all_nutrients.extend(nutrients);
                }
                // ... otros platforms
            }
        }
        
        Ok(all_nutrients)
    }
}
```

**Dependencias:**
- `yt-dlp` CLI instalado en sistema
  - Install: `pip install yt-dlp` (Python required)
  - Fallback: `youtube-dl` (legacy, menos features)
- `tokio` (async runtime)
- `serde` + `serde_json` (parsing)
- `reqwest` (download subtitles)
- `regex` (video ID extraction)

**Rate Limiting:**
- yt-dlp maneja throttling internamente
- Nosotros agregamos: max 5 concurrent requests (Semaphore)
- Exponential backoff si yt-dlp falla (retry 3x)

#### 3.2.2 Spotify Extractor: Musical Memory Engine (v3.0)

**üéµ Visi√≥n Estrat√©gica:** NO solo metadata - construir **memoria musical profunda** que entienda teor√≠a, historia, contexto cultural.

##### ¬øPor qu√© Musical Memory?

Cuando alguien comparte "Bohemian Rhapsody" vs "Despacito" vs "Clair de Lune", NO solo comparten:
- G√©nero o BPM
- Valence/energy numbers

Comparten:
- **Teor√≠a Musical:** Modulaciones complejas, estructura sinf√≥nica
- **ADN Musical:** "Queen influy√≥ a Muse, Radiohead, 30STM"
- **Contexto Cultural:** Revolucion√≥ rock progresivo en 1975
- **Momento Emocional:** ¬øPor qu√© HOY? ¬øCorrelaci√≥n con mood/eventos?

**La Spotify API nos da n√∫meros. Nosotros queremos ENTENDER la m√∫sica.**

##### Phase Approach

```yaml
v1.0-v2.0 (Current Implementation):
  Scope: Placeholder b√°sico (Spotify API metadata + audio features)
  Goal: Functional extraction sin bloquear MVP
  
v3.0 (Musical Memory Engine):
  Scope: An√°lisis profundo
  Sources:
    - Spotify API (metadata, audio features)
    - LLM (teor√≠a musical, an√°lisis l√≠rico)
    - MusicBrainz (relaciones, influencias)
    - Last.fm (tags comunitarios, similar artists)
    - Genius (lyrics, annotations)
    - Wikipedia (historia, contexto cultural)
    - News APIs (eventos actuales del artista)
  Goal: Entender m√∫sica profundamente (theory, history, relationships)
```

##### Implementaci√≥n v1.0 (Placeholder B√°sico)

```rust
// Flow: Spotify URL ‚Üí Basic Metadata Extraction
async fn extract_spotify_content(url: &str) -> Result<SpotifyContent> {
    // Step 1: Check cache (30 d√≠as - m√∫sica no cambia)
    if let Some(cached) = CACHE.get(url).await? {
        if !cached.is_expired() {
            return Ok(cached.content);
        }
    }
    
    // Step 2: Extract track/playlist/album ID
    let (content_type, spotify_id) = parse_spotify_url(url)?;
    //   - Regex: /track/(\w+), /playlist/(\w+), /album/(\w+), /artist/(\w+)
    
    // Step 3: Authenticate with Spotify API (OAuth Client Credentials)
    let token = get_spotify_token().await?;
    //   - Cache token (expires 3600s)
    //   - Client ID + Secret from env vars
    
    // Step 4: Fetch metadata
    let content = match content_type {
        SpotifyType::Track => {
            // Parallel fetch: metadata + audio features
            let (track, audio_features) = tokio::try_join!(
                fetch_track(&spotify_id, &token),
                fetch_audio_features(&spotify_id, &token),
            )?;
            
            SpotifyContent::Track {
                name: track.name,
                artist: track.artists[0].name.clone(),
                album: track.album.name,
                release_year: extract_year(&track.album.release_date),
                duration: Duration::from_millis(track.duration_ms),
                genres: track.artists[0].genres.clone(),  // From artist endpoint
                audio_features: AudioFeatures {
                    valence: audio_features.valence,          // 0.0-1.0 (happiness)
                    energy: audio_features.energy,            // 0.0-1.0 (intensity)
                    danceability: audio_features.danceability,
                    tempo: audio_features.tempo,              // BPM
                    key: note_from_key(audio_features.key),   // "C", "C#", "D"...
                    mode: if audio_features.mode == 1 { "major" } else { "minor" },
                    time_signature: audio_features.time_signature,
                    acousticness: audio_features.acousticness,
                    instrumentalness: audio_features.instrumentalness,
                },
                // Basic emotional inference (v1.0)
                emotional_inference: infer_emotion(&audio_features),
            }
        }
        SpotifyType::Playlist => {
            let playlist = fetch_playlist(&spotify_id, &token).await?;
            SpotifyContent::Playlist {
                name: playlist.name,
                description: playlist.description,
                total_tracks: playlist.tracks.total,
                tracks: playlist.tracks.items.into_iter()
                    .take(50)  // Limit para performance
                    .map(|t| TrackSummary {
                        name: t.track.name,
                        artist: t.track.artists[0].name.clone(),
                    })
                    .collect(),
                total_duration: calculate_total_duration(&playlist.tracks),
                genres: extract_playlist_genres(&playlist),  // Aggregate from tracks
            }
        }
        SpotifyType::Album => {
            let album = fetch_album(&spotify_id, &token).await?;
            SpotifyContent::Album {
                name: album.name,
                artist: album.artists[0].name.clone(),
                release_date: album.release_date,
                release_year: extract_year(&album.release_date),
                tracks: album.tracks.items.into_iter()
                    .map(|t| t.name)
                    .collect(),
                total_tracks: album.total_tracks,
                genres: album.genres,
            }
        }
    };
    
    // Step 5: Cache result (30 d√≠as)
    CACHE.set(url, &content, Duration::from_days(30)).await?;
    
    Ok(content)
}

/// Basic emotional inference (v1.0 - simple heuristics)
fn infer_emotion(features: &AudioFeatures) -> String {
    match (features.valence, features.energy) {
        (v, e) if v > 0.7 && e > 0.7 => "energetic, happy".to_string(),
        (v, e) if v > 0.7 && e < 0.4 => "calm, content".to_string(),
        (v, e) if v < 0.4 && e > 0.7 => "angry, intense".to_string(),
        (v, e) if v < 0.4 && e < 0.4 => "melancholic, sad".to_string(),
        _ => "neutral".to_string(),
    }
}
```

**Dependencias v1.0:**
- `rspotify` crate (Spotify API wrapper Rust)
- Spotify Developer credentials (Client ID + Secret)
- OAuth2 Client Credentials Flow

**Rate Limiting:**
- 180 requests/minute per app
- Token caching (3600s lifetime)
- Semaphore: max 5 concurrent requests

**Limitaciones v1.0:**
- ‚ùå No teor√≠a musical profunda
- ‚ùå No contexto hist√≥rico/cultural
- ‚ùå No an√°lisis l√≠rico
- ‚ùå No relaciones/influencias
- ‚ùå No eventos actuales

**Suficiente para v1.0-v2.0. Musical Memory Engine se implementa en v3.0 cuando tengamos usuarios activos.**

#### 3.2.3 Webpage Extractor: LLM Delegation Strategy

**üåê Decisi√≥n Estrat√©gica:** NO reinventar scrapers - **delegar a LLMs** (GPT-4o-mini).

##### ¬øPor qu√© LLMs en vez de Scrapers?

**Reflexi√≥n: ¬øCu√°ntos scrapers has visto morir?**

HTML cambia. Sitios a√±aden anti-bot. JavaScript frameworks evolucionan. Paywalls aparecen.

```yaml
Approach Tradicional (Scrapers):
  Pros: Offline, gratis, control total
  Contras:
    - Mantenimiento INFINITO (HTML cambia constantemente)
    - Falla con JavaScript din√°mico
    - Anti-bot measures (captchas, rate limits)
    - Regex hell para parsing
    - 1000+ casos edge (encoding, malformed HTML)

Approach LLM (GPT-4o-mini):
  Pros:
    - Funciona con CUALQUIER HTML
    - Inteligente (distingue content vs noise)
    - Mantenimiento: 0 (OpenAI se encarga)
    - Semantic understanding (extrae topics, sentiment)
    - Costo: ~$0.01/article
  Contras:
    - Requiere internet
    - Latencia: ~2-3 segundos
    - Privacy: HTML va a OpenAI (mitigable con cache)

Decisi√≥n: ‚úÖ LLM approach
  Why:
    - $0.01/article << developer time maintaining scrapers
    - 7-day cache ‚Üí solo pagamos 1x por URL
    - 5-10 links/user/day ‚Üí $0.05-0.10/user/day
    - 100 users = $5-10/day = $150-300/month
    - Alternative: $2000+ developer hours on scrapers
  
  Philosophy: "No reinventar la rueda. Delegar a quien lo hace mejor."
```

##### Implementaci√≥n LLM-Based

```rust
// Flow: Generic URL ‚Üí LLM Content Extraction
async fn extract_webpage_content(url: &str) -> Result<WebpageContent> {
    // Step 1: Check cache (7 d√≠as - articles don't change)
    let cache_key = sha256(url);
    if let Some(cached) = CACHE.get(&cache_key).await? {
        tracing::info!("Cache HIT: {}", url);
        return Ok(cached);
    }
    
    tracing::info!("Cache MISS: {}", url);
    
    // Step 2: Fetch HTML
    let client = reqwest::Client::builder()
        .user_agent("Mozilla/5.0 (compatible; Bitacora/1.0)")
        .timeout(Duration::from_secs(15))
        .redirect(reqwest::redirect::Policy::limited(5))
        .build()?;
    
    let response = client.get(url).send().await?;
    
    if !response.status().is_success() {
        return Err(ContentExtractionError::HttpError {
            url: url.to_string(),
            status: response.status().as_u16(),
        });
    }
    
    let html = response.text().await?;
    
    // Step 3: Preprocess (remove scripts/styles, truncate to 500KB)
    let clean_html = preprocess_html(&html);
    
    // Step 4: LLM Extraction (GPT-4o-mini)
    let prompt = format!(
        r#"Extract structured information from this webpage HTML.

HTML:
{}

Extract and return ONLY valid JSON (no markdown, no explanation):
{{
  "title": "Article title",
  "author": "Author name or null",
  "published_date": "YYYY-MM-DD or null",
  "main_content": "Full article text (clean, no ads/navigation)",
  "summary": "1-2 sentence summary",
  "language": "en/es/fr/etc",
  "topics": ["topic1", "topic2", "topic3"],
  "sentiment": "positive/negative/neutral",
  "content_type": "article/blog/news/tutorial/opinion",
  "key_points": ["point 1", "point 2", "point 3"]
}}

Rules:
- Ignore navigation, ads, footers, sidebars
- Extract only main article/blog content
- If no clear content, return null for main_content
- Topics: 3-5 main themes
- Return ONLY JSON, no markdown formatting"#,
        clean_html
    );
    
    let llm_client = LLMClient::new();
    let json_response = llm_client.chat_with_json(&prompt).await?;
    
    // Step 5: Parse LLM response
    let extracted: ExtractedData = serde_json::from_str(&json_response)
        .map_err(|e| ContentExtractionError::LLMParseError {
            details: e.to_string(),
        })?;
    
    // Step 6: Post-processing
    let word_count = extracted.main_content.split_whitespace().count();
    let reading_time = (word_count / 200).max(1);  // 200 WPM
    
    let domain = url::Url::parse(url)
        .ok()
        .and_then(|u| u.host_str().map(String::from));
    
    let content = WebpageContent {
        url: url.to_string(),
        title: extracted.title,
        author: extracted.author,
        published_date: extracted.published_date,
        main_content: extracted.main_content,
        summary: extracted.summary,
        language: extracted.language,
        topics: extracted.topics,
        sentiment: extracted.sentiment,
        content_type: extracted.content_type,
        key_points: extracted.key_points,
        domain,
        word_count,
        reading_time_minutes: reading_time,
        extraction_method: "llm".to_string(),
        model: "gpt-4o-mini".to_string(),
        extracted_at: chrono::Utc::now(),
    };
    
    // Step 7: Cache result (7 d√≠as)
    CACHE.set(&cache_key, &content, Duration::from_days(7)).await?;
    
    Ok(content)
}

/// Remove scripts, styles, noscript
fn preprocess_html(html: &str) -> String {
    use regex::Regex;
    
    let patterns = [
        r"<script[^>]*>.*?</script>",
        r"<style[^>]*>.*?</style>",
        r"<noscript[^>]*>.*?</noscript>",
    ];
    
    let mut clean = html.to_string();
    for pattern in &patterns {
        let re = Regex::new(pattern).unwrap();
        clean = re.replace_all(&clean, "").to_string();
    }
    
    // Truncate to 500KB if larger (LLM token limits)
    const MAX_SIZE: usize = 500 * 1024;
    if clean.len() > MAX_SIZE {
        tracing::warn!("HTML too large ({}KB), truncating", clean.len() / 1024);
        clean.truncate(MAX_SIZE);
    }
    
    clean
}

#[derive(Debug, Deserialize)]
struct ExtractedData {
    title: String,
    author: Option<String>,
    published_date: Option<String>,
    main_content: String,
    summary: String,
    language: String,
    topics: Vec<String>,
    sentiment: String,
    content_type: String,
    key_points: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WebpageContent {
    pub url: String,
    pub title: String,
    pub author: Option<String>,
    pub published_date: Option<String>,
    pub main_content: String,
    pub summary: String,
    pub language: String,
    pub topics: Vec<String>,
    pub sentiment: String,
    pub content_type: String,
    pub key_points: Vec<String>,
    pub domain: Option<String>,
    pub word_count: usize,
    pub reading_time_minutes: usize,
    pub extraction_method: String,  // "llm"
    pub model: String,              // "gpt-4o-mini"
    pub extracted_at: chrono::DateTime<chrono::Utc>,
}
```

**Dependencias:**
- `reqwest` (HTTP client) - ‚úÖ already in project
- `LLMClient` (GPT-4o-mini) - ‚úÖ already exists in `src/multi_agent/llm_client.rs`
- `serde_json` (parsing) - ‚úÖ already in project
- `regex` (HTML preprocessing) - ‚úÖ already in project
- `sha2` (cache keys) - ‚úÖ already in project

**Cost Analysis (GPT-4o-mini):**
```yaml
Pricing (Nov 2025):
  Input: $0.150 / 1M tokens
  Output: $0.600 / 1M tokens

Average Article:
  HTML: ~100KB ‚âà 25,000 tokens
  Output: ~1,000 tokens (JSON)
  Cost: $0.00375 + $0.00060 ‚âà $0.005 per article

Real Usage (100 users):
  - 5-10 links/user/day = 500-1000 extractions/day
  - Cache hit rate: 60% (re-shared links)
  - Actual: 200-400 extractions/day
  - Cost: $1-2/day = $30-60/month
  
vs Custom Scrapers:
  - Developer time: 40+ hours initial
  - Maintenance: 5+ hours/month (fixing)
  - Opportunity cost: HUGE
  
Conclusion: LLM approach ~100x cheaper considering developer time
```

**Ventajas vs Scrapers:**
- ‚úÖ **Robustez:** Entiende estructura sem√°ntica, no depende de HTML espec√≠fico
- ‚úÖ **Mantenimiento:** Zero (OpenAI mantiene modelo)
- ‚úÖ **Cobertura:** Universal (cualquier HTML structure)
- ‚úÖ **Intelligence:** Extrae topics, sentiment, key points (no solo texto)
- ‚ö†Ô∏è **Latencia:** ~2-3s (vs ~100ms scrapers) - mitigado con cache
- ‚ö†Ô∏è **Privacy:** HTML va a OpenAI - mitigado con 7-day cache (reduce exposici√≥n)

#### 3.2.4 Audio File Extractor

```rust
// Flow: Audio URL (.mp3, .wav) ‚Üí Transcription
async fn extract_audio_content(url: &str) -> Result<AudioContent> {
    // Step 1: Check cache
    if let Some(cached) = CACHE.get(url).await? {
        if !cached.is_expired() {
            return Ok(cached.content);
        }
    }
    
    // Step 2: Download audio (streaming, no full download)
    let audio_bytes = download_audio_partial(url, MAX_AUDIO_SIZE).await?;
    
    // Step 3: Detect duration (ffprobe)
    let duration = detect_audio_duration(&audio_bytes)?;
    
    // Step 4: Transcribe with Whisper API
    let transcript = if duration < Duration::from_secs(600) {  // <10 min
        transcribe_whisper_api(&audio_bytes).await?
    } else {
        None  // Too expensive, skip transcription
    };
    
    // Step 5: Build content
    let content = AudioContent {
        url: url.to_string(),
        duration,
        transcript,
        file_size: audio_bytes.len(),
        format: detect_audio_format(url)?,
    };
    
    // Step 6: Cache result
    CACHE.set(url, &content, Duration::from_days(7)).await?;
    
    Ok(content)
}
```

**Dependencias:**
- `ffmpeg` CLI (audio metadata)
- Whisper API (OpenAI) - $0.006/minute
- Fallback: local Whisper (whisper-rs crate)

**Cost Management:**
- Solo transcribir audios <10 min
- Budget tracking: max $5/day en Whisper API
- Alert si se excede budget

#### 3.2.5 Video File Extractor

```rust
// Flow: Video URL (.mp4, .webm) ‚Üí Metadata + Audio Extraction
async fn extract_video_content(url: &str) -> Result<VideoContent> {
    // Step 1: Check cache
    if let Some(cached) = CACHE.get(url).await? {
        if !cached.is_expired() {
            return Ok(cached.content);
        }
    }
    
    // Step 2: Extract metadata (ffprobe)
    let metadata = tokio::process::Command::new("ffprobe")
        .args(&[
            "-v", "quiet",
            "-print_format", "json",
            "-show_format",
            "-show_streams",
            url,
        ])
        .output()
        .await?;
    
    let json: VideoMetadata = serde_json::from_slice(&metadata.stdout)?;
    
    // Step 3: Extract audio track
    let audio_extracted = extract_audio_from_video(url).await?;
    
    // Step 4: Transcribe audio (if <10 min)
    let transcript = if json.duration < 600.0 {
        transcribe_whisper_api(&audio_extracted).await?
    } else {
        None
    };
    
    // Step 5: Build content
    let content = VideoContent {
        url: url.to_string(),
        duration: Duration::from_secs_f64(json.duration),
        resolution: format!("{}x{}", json.width, json.height),
        codec: json.codec_name,
        file_size: json.size,
        transcript,
    };
    
    // Step 6: Cache result
    CACHE.set(url, &content, Duration::from_days(7)).await?;
    
    Ok(content)
}
```

**Dependencias:**
- `ffmpeg` + `ffprobe` CLI (video processing)
- Whisper API (audio transcription)

**Limitations:**
- No descargar video completo (solo metadata + audio)
- Skip videos >100MB

---

## 4Ô∏è‚É£ MAPEO ACTUAL vs IDEAL

### 4.1 Estado Actual (v1.0 - B√ÅSICO)

```rust
// src/data_import/extraction.rs (actual)
ClassifiedLink {
    url: String,                      // ‚úÖ Implementado
    platform: Platform,               // ‚úÖ Implementado
    category: ContentCategory,        // ‚úÖ Implementado
    intent: LinkIntent,               // ‚úÖ Implementado
    context: String,                  // ‚úÖ Implementado
    confidence: f64,                  // ‚úÖ Implementado
    // ‚ùå NO HAY: metadata enriquecida
}
```

### 4.2 Estado Ideal (v1.1 - ENRIQUECIDO)

```rust
// Objetivo v1.1
ClassifiedLink {
    url: String,
    platform: Platform,
    category: ContentCategory,
    intent: LinkIntent,
    context: String,
    confidence: f64,
    // ‚úÖ NUEVO: Content enrichment
    enriched_metadata: Option<EnrichedMetadata>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EnrichedMetadata {
    YouTube(YouTubeContent),
    Spotify(SpotifyContent),
    Webpage(WebpageContent),
    Audio(AudioContent),
    Video(VideoContent),
}
```

### 4.3 Gap Analysis

| Feature                  | v1.0 (Actual) | v1.1 (Target) | Gap  |
|--------------------------|---------------|---------------|------|
| URL extraction           | ‚úÖ            | ‚úÖ            | 0%   |
| Platform classification  | ‚úÖ            | ‚úÖ            | 0%   |
| Intent inference         | ‚úÖ            | ‚úÖ            | 0%   |
| YouTube metadata         | ‚ùå            | ‚úÖ            | 100% |
| YouTube transcripts      | ‚ùå            | ‚úÖ            | 100% |
| Spotify metadata         | ‚ùå            | ‚úÖ            | 100% |
| Webpage scraping         | ‚ùå            | ‚úÖ            | 100% |
| Audio transcription      | ‚ùå            | ‚úÖ            | 100% |
| Video metadata           | ‚ùå            | ‚úÖ            | 100% |
| Caching layer            | ‚ùå            | ‚úÖ            | 100% |
| Rate limiting            | ‚ùå            | ‚úÖ            | 100% |

**Overall Gap: 70% features missing**

---

## 5Ô∏è‚É£ DETECCI√ìN DE PROBLEMAS

> **Filosof√≠a:** *"Un problema bien formulado es medio problema resuelto. Un problema anticipado es un problema evitado."*

### 5.0 La Naturaleza de los Problemas en Content Extraction

**Reflexi√≥n antes del c√≥digo:**

Content extraction no es "fetch y parse". Es un **sistema distribuido impl√≠cito**:

```
Tu c√≥digo ‚Üí Internet ‚Üí YouTube servers
                     ‚Üí Spotify API
                     ‚Üí Random websites
                     ‚Üí OpenAI Whisper
```

Cada nodo puede fallar de maneras √∫nicas:
- YouTube: throttling silencioso (200 OK pero data corrupta)
- Spotify: token expiry mid-request
- Websites: HTML din√°mico (JavaScript-rendered)
- Whisper: quota exceeded sin warning

**Aprendizaje:** No dise√±es para el **happy path**. Dise√±a para el **chaos path**.

### 5.1 Problemas T√©cnicos

#### Problema 1: Dependencias Externas

**Descripci√≥n:**
- `yt-dlp` CLI debe estar instalado en sistema
- `ffmpeg` CLI debe estar instalado
- Spotify API requiere credenciales

**Impacto:** üî¥ CR√çTICO
- Si `yt-dlp` no est√° instalado ‚Üí YouTube extraction falla
- Si `ffmpeg` no est√° ‚Üí Audio/Video extraction falla

**Soluci√≥n:**
```rust
// Check dependencies at startup
pub fn check_dependencies() -> Result<DependencyStatus> {
    let mut status = DependencyStatus::default();
    
    // Check yt-dlp
    if Command::new("yt-dlp").arg("--version").output().is_ok() {
        status.ytdlp_available = true;
    } else {
        eprintln!("‚ö†Ô∏è  yt-dlp not found. YouTube extraction disabled.");
        eprintln!("   Install: pip install yt-dlp");
    }
    
    // Check ffmpeg
    if Command::new("ffmpeg").arg("-version").output().is_ok() {
        status.ffmpeg_available = true;
    } else {
        eprintln!("‚ö†Ô∏è  ffmpeg not found. Audio/Video extraction disabled.");
        eprintln!("   Install: sudo apt install ffmpeg");
    }
    
    // Check Spotify credentials
    if env::var("SPOTIFY_CLIENT_ID").is_ok() && env::var("SPOTIFY_CLIENT_SECRET").is_ok() {
        status.spotify_available = true;
    } else {
        eprintln!("‚ö†Ô∏è  Spotify credentials not found. Spotify extraction disabled.");
        eprintln!("   Set: SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET");
    }
    
    Ok(status)
}
```

#### Problema 2: Costos de APIs

**Descripci√≥n:**
- Whisper API: $0.006/minuto
- Spotify API: Gratis (con rate limits)
- YouTube: Gratis v√≠a yt-dlp (no API quota)

**Impacto:** üü° MODERADO
- Si usuario tiene 100 audios de 5 min ‚Üí $3.00
- Budget puede agotarse r√°pidamente

**Soluci√≥n:**
```rust
pub struct ExtractionBudget {
    pub max_whisper_cost_usd: f32,  // Default: $5.00
    pub current_cost: f32,
    pub max_audio_duration_sec: u64, // Default: 600 (10 min)
}

impl ExtractionBudget {
    pub fn can_transcribe(&self, duration_sec: u64) -> bool {
        let cost = (duration_sec as f32 / 60.0) * 0.006;
        (self.current_cost + cost) <= self.max_whisper_cost_usd
    }
}
```

#### Problema 3: Rate Limiting

**Descripci√≥n:**
- YouTube puede throttle requests si no respetamos rate limits
- Spotify API: 180 req/min
- Websites pueden bloquear scraping

**Impacto:** üü° MODERADO
- Extraction puede fallar
- Necesitamos retry logic

**Soluci√≥n:**
```rust
use tokio::time::{sleep, Duration};

pub struct RateLimiter {
    pub max_concurrent: usize,       // Default: 5
    pub delay_between_requests: Duration, // Default: 200ms
}

impl RateLimiter {
    pub async fn throttle(&self) {
        sleep(self.delay_between_requests).await;
    }
}
```

#### Problema 4: Parsing Errors

**Descripci√≥n:**
- Websites tienen HTML diferente
- YouTube captions en m√∫ltiples formatos (VTT, SRT, TXT)
- Spotify API puede retornar null values

**Impacto:** üü¢ BAJO
- Extraction falla para ese URL espec√≠fico
- No afecta otros URLs

**Soluci√≥n:**
```rust
// Graceful degradation
match extract_youtube_content(url).await {
    Ok(content) => Some(content),
    Err(e) => {
        eprintln!("‚ö†Ô∏è  Failed to extract {}: {}", url, e);
        None  // Continue with other URLs
    }
}
```

### 5.2 Problemas de Dise√±o

#### Problema A: Cache Invalidation

**Descripci√≥n:**
- Videos pueden ser eliminados
- Playlists pueden cambiar
- Websites pueden actualizar contenido

**Soluci√≥n:**
- TTL de 7 d√≠as
- Soft invalidation (re-fetch si falla)
- Manual cache clear command

#### Problema B: Privacy

**Descripci√≥n:**
- Transcripts pueden contener informaci√≥n sensible
- URLs pueden tener tokens de sesi√≥n

**Soluci√≥n:**
- No guardar tokens en cache
- Anonymize transcripts (detectar nombres, emails)
- User consent para transcripciones

---

## 6Ô∏è‚É£ PLAN DE ACCI√ìN

> **Filosof√≠a Theremin:** *"Silencio antes de ruido. Dise√±o antes de c√≥digo. Tests antes de features."*

### 6.0 Estrategia de Implementaci√≥n

**Por qu√© este orden espec√≠fico:**

```
Phase 1 (Infrastructure) PRIMERO porque:
  ‚îú‚îÄ Cache es cr√≠tico (evita re-fetch costoso)
  ‚îú‚îÄ Rate limiting evita bans
  ‚îî‚îÄ Sin esto, Phases 2-6 son ca√≥ticos

Phase 2 (YouTube) SEGUNDO porque:
  ‚îú‚îÄ Mayor volumen esperado (40% de links)
  ‚îú‚îÄ yt-dlp es estable y bien documentado
  ‚îî‚îÄ Valida arquitectura end-to-end

Phase 3 (Webpage) TERCERO porque:
  ‚îú‚îÄ M√°s simple que multimedia
  ‚îú‚îÄ Prueba HTML parsing
  ‚îî‚îÄ No requiere APIs externas

Phase 4-6 (Audio/Video/Spotify) AL FINAL porque:
  ‚îú‚îÄ Requieren budget management (Phase 1)
  ‚îú‚îÄ Requieren caching agresivo (Phase 1)
  ‚îî‚îÄ Menos volumen, menos prioridad
```

**Principio:** Construir **fundaci√≥n robusta** antes de **features complejas**.

### 6.1 Implementation Roadmap

#### Phase 1: Infrastructure (v1.1.0 - 2 weeks)

**Objetivos:**
- Crear m√≥dulo `content_extractors/`
- Implementar cache layer (SQLite)
- Dependency checking
- Rate limiting b√°sico

**Tasks:**
```yaml
Task 7.x.3.8.1: Content Extractors Infrastructure
  Priority: HIGH
  Effort: 8h
  Files:
    - src/data_import/content_extractors/mod.rs
    - src/data_import/content_extractors/cache.rs
    - src/data_import/content_extractors/rate_limiter.rs
  Dependencies:
    - sqlx (SQLite)
    - tokio
  Tests:
    - test_cache_basic.rs
    - test_rate_limiter.rs
```

#### Phase 2: YouTube Extractor (v1.1.1 - 1 week)

**Objetivos:**
- Integraci√≥n yt-dlp
- Parsing metadata JSON
- Subtitle extraction (VTT/SRT)

**Tasks:**
```yaml
Task 7.x.3.8.2: YouTube Content Extractor
  Priority: HIGH
  Effort: 12h
  Files:
    - src/data_import/content_extractors/youtube.rs
  Dependencies:
    - yt-dlp CLI
    - serde_json
  Tests:
    - test_youtube_extractor.rs (real videos)
```

#### Phase 3: Webpage Scraper (v1.1.2 - 1 week)

**Objetivos:**
- HTML parsing
- Open Graph extraction
- Main content extraction (Readability)

**Tasks:**
```yaml
Task 7.x.3.8.3: Webpage Content Extractor
  Priority: MEDIUM
  Effort: 10h
  Files:
    - src/data_import/content_extractors/webpage.rs
  Dependencies:
    - scraper
    - lingua-rs
  Tests:
    - test_webpage_extractor.rs
```

#### Phase 4: Audio Transcription (v1.1.3 - 1 week)

**Objetivos:**
- Whisper API integration
- Budget tracking
- Local Whisper fallback

**Tasks:**
```yaml
Task 7.x.3.8.4: Audio Content Extractor
  Priority: MEDIUM
  Effort: 10h
  Files:
    - src/data_import/content_extractors/audio.rs
  Dependencies:
    - reqwest (Whisper API)
    - whisper-rs (local fallback)
  Tests:
    - test_audio_extractor.rs
```

#### Phase 5: Spotify Integration (v1.1.4 - 3 days)

**Objetivos:**
- Spotify Web API
- OAuth token management
- Playlist parsing

**Tasks:**
```yaml
Task 7.x.3.8.5: Spotify Content Extractor
  Priority: LOW
  Effort: 6h
  Files:
    - src/data_import/content_extractors/spotify.rs
  Dependencies:
    - rspotify
  Tests:
    - test_spotify_extractor.rs
```

#### Phase 6: Video Processing (v1.2.0 - 1 week)

**Objetivos:**
- ffmpeg integration
- Audio extraction
- Metadata parsing

**Tasks:**
```yaml
Task 7.x.3.8.6: Video Content Extractor
  Priority: LOW
  Effort: 8h
  Files:
    - src/data_import/content_extractors/video.rs
  Dependencies:
    - ffmpeg CLI
  Tests:
    - test_video_extractor.rs
```

### 6.2 Milestones

```yaml
Milestone 1: Content Extraction Infrastructure (v1.1.0)
  Date Target: 2025-12-15
  Deliverables:
    - Cache layer functional
    - Rate limiting working
    - Dependency checking
  Validation:
    - Cache hit rate >80%
    - No rate limit errors

Milestone 2: YouTube + Webpage Extraction (v1.1.2)
  Date Target: 2025-12-30
  Deliverables:
    - YouTube videos: metadata + transcripts
    - Websites: Open Graph + main text
  Validation:
    - 90%+ extraction success rate
    - <5s per URL extraction

Milestone 3: Audio/Video + Spotify (v1.2.0)
  Date Target: 2026-01-15
  Deliverables:
    - Audio transcription (Whisper)
    - Video metadata (ffmpeg)
    - Spotify tracks/playlists
  Validation:
    - Budget tracking <$5/day
    - Spotify API working
```

---

## 7Ô∏è‚É£ VALIDACI√ìN POST-CAMBIO

### 7.1 Test Cases

#### Test 1: YouTube Extraction Accuracy

```rust
#[tokio::test]
async fn test_youtube_extraction_complete() {
    let extractor = YouTubeExtractor::new();
    
    let url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ";
    let content = extractor.extract(url).await.unwrap();
    
    // Validate metadata
    assert_eq!(content.title, "Rick Astley - Never Gonna Give You Up");
    assert!(content.duration.as_secs() > 0);
    assert!(content.author.contains("Rick Astley"));
    
    // Validate transcript
    assert!(content.transcript.is_some());
    let transcript = content.transcript.unwrap();
    assert!(transcript.contains("never gonna give you up"));
}
```

#### Test 2: Cache Hit Rate

```rust
#[tokio::test]
async fn test_cache_performance() {
    let cache = ContentCache::new("test.db").await.unwrap();
    let url = "https://example.com/test";
    
    // First call: cache miss
    let start = Instant::now();
    let content1 = extract_with_cache(&cache, url).await.unwrap();
    let duration1 = start.elapsed();
    
    // Second call: cache hit
    let start = Instant::now();
    let content2 = extract_with_cache(&cache, url).await.unwrap();
    let duration2 = start.elapsed();
    
    // Cache should be 10x+ faster
    assert!(duration2 < duration1 / 10);
    assert_eq!(content1, content2);
}
```

#### Test 3: Rate Limiting

```rust
#[tokio::test]
async fn test_rate_limiter_throttle() {
    let limiter = RateLimiter::new(5, Duration::from_millis(200));
    
    let start = Instant::now();
    
    // Make 10 requests
    for i in 0..10 {
        limiter.acquire().await;
        println!("Request {}", i);
    }
    
    let duration = start.elapsed();
    
    // Should take ~2 seconds (10 * 200ms)
    assert!(duration >= Duration::from_millis(1800));
    assert!(duration <= Duration::from_millis(2200));
}
```

#### Test 4: Budget Tracking

```rust
#[tokio::test]
async fn test_whisper_budget_enforcement() {
    let mut budget = ExtractionBudget::new(5.0); // $5 max
    
    // Simulate 10 min audio ($0.06)
    assert!(budget.can_transcribe(600));
    budget.record_cost(600);
    
    // Simulate 100 audios (would cost $6.00)
    for _ in 0..100 {
        if budget.can_transcribe(60) {
            budget.record_cost(60);
        }
    }
    
    // Should stop before exceeding budget
    assert!(budget.current_cost <= 5.0);
}
```

### 7.2 Performance Benchmarks

```yaml
Target Performance (v1.1):
  YouTube extraction: <5s per video
  Webpage scraping: <2s per page
  Cache lookup: <10ms
  Parallel extraction (10 URLs): <15s total
  Memory usage: <500MB peak

Validation Command:
  cargo run --example benchmark_content_extraction
```

---

## 8Ô∏è‚É£ HERRAMIENTAS AUTOM√ÅTICAS

### 8.1 Dependency Checker Script

```bash
#!/bin/bash
# scripts/check_content_extraction_deps.sh

echo "üîç Checking Content Extraction Dependencies..."

# Check yt-dlp
if command -v yt-dlp &> /dev/null; then
    echo "‚úÖ yt-dlp: $(yt-dlp --version)"
else
    echo "‚ùå yt-dlp: NOT FOUND"
    echo "   Install: pip install yt-dlp"
fi

# Check ffmpeg
if command -v ffmpeg &> /dev/null; then
    echo "‚úÖ ffmpeg: $(ffmpeg -version | head -n1)"
else
    echo "‚ùå ffmpeg: NOT FOUND"
    echo "   Install: sudo apt install ffmpeg"
fi

# Check ffprobe
if command -v ffprobe &> /dev/null; then
    echo "‚úÖ ffprobe: $(ffprobe -version | head -n1)"
else
    echo "‚ùå ffprobe: NOT FOUND"
fi

# Check Spotify credentials
if [[ -n "$SPOTIFY_CLIENT_ID" ]] && [[ -n "$SPOTIFY_CLIENT_SECRET" ]]; then
    echo "‚úÖ Spotify credentials: CONFIGURED"
else
    echo "‚ö†Ô∏è  Spotify credentials: NOT CONFIGURED (optional)"
fi

# Check OpenAI API key (Whisper)
if [[ -n "$OPENAI_API_KEY" ]]; then
    echo "‚úÖ OpenAI API key: CONFIGURED"
else
    echo "‚ö†Ô∏è  OpenAI API key: NOT CONFIGURED (audio transcription disabled)"
fi
```

### 8.2 Cache Management CLI

```bash
# View cache stats
bitacora cache stats

# Clear expired entries
bitacora cache clean

# Clear all cache
bitacora cache clear --all

# Inspect cached URL
bitacora cache inspect "https://youtube.com/watch?v=abc"
```

---

## 9Ô∏è‚É£ EST√ÅNDARES GLOBALES

> **Filosof√≠a:** *"El c√≥digo que compila NO es suficiente. El c√≥digo que COMUNICA es excelente."*

### 9.0 Principios de C√≥digo Limpio en Content Extraction

**Patr√≥n sobre implementaci√≥n:**

```rust
// ‚ùå C√≥digo que funciona (pero no canta)
async fn get_yt(u: &str) -> Result<String> {
    let x = cmd("yt-dlp").arg(u).output().await?;
    Ok(String::from_utf8(x.stdout)?)
}

// ‚úÖ C√≥digo que comunica intenci√≥n
/// Extrae metadata de YouTube usando yt-dlp.
/// 
/// # Arguments
/// * `url` - URL completa del video (https://youtube.com/watch?v=...)
/// 
/// # Returns
/// * `YouTubeContent` con t√≠tulo, descripci√≥n, duraci√≥n, transcript
/// 
/// # Errors
/// * `DependencyMissing` si yt-dlp no est√° instalado
/// * `RateLimitExceeded` si YouTube throttle la request
/// * `VideoPrivate` si el video no es p√∫blico
async fn extract_youtube_metadata(url: &str) 
    -> Result<YouTubeContent, ContentExtractionError> 
{
    // Step 1: Validate URL format
    let video_id = extract_video_id(url)
        .ok_or(ContentExtractionError::InvalidUrl { url: url.to_string() })?;
    
    // Step 2: Check cache first (avoid unnecessary API calls)
    if let Some(cached) = self.cache.get(url).await? {
        return Ok(cached);
    }
    
    // Step 3: Call yt-dlp
    let output = tokio::process::Command::new("yt-dlp")
        .args(&["--dump-json", "--skip-download", url])
        .output()
        .await
        .map_err(|e| ContentExtractionError::DependencyMissing { 
            dependency: "yt-dlp".to_string(),
            install_hint: "pip install yt-dlp".to_string(),
        })?;
    
    // Step 4: Parse response
    let metadata: YouTubeMetadata = serde_json::from_slice(&output.stdout)?;
    
    // Step 5: Build content object
    let content = YouTubeContent {
        video_id,
        title: metadata.title,
        duration: Duration::from_secs(metadata.duration),
        // ... resto de campos
    };
    
    // Step 6: Cache for future use
    self.cache.set(url, &content, Duration::from_days(7)).await?;
    
    Ok(content)
}
```

**Diferencia clave:**
- Nombres descriptivos (`extract_youtube_metadata` vs `get_yt`)
- Documentaci√≥n de contrato (args, returns, errors)
- Pasos comentados (narrative flow)
- Error handling expl√≠cito (qu√© puede fallar y por qu√©)

### 9.1 Error Handling

```rust
#[derive(Debug, thiserror::Error)]
pub enum ContentExtractionError {
    #[error("Platform not supported: {platform}")]
    UnsupportedPlatform { platform: String },
    
    #[error("Dependency missing: {dependency}")]
    DependencyMissing { dependency: String },
    
    #[error("Rate limit exceeded for {url}")]
    RateLimitExceeded { url: String },
    
    #[error("Budget exceeded: ${current:.2} / ${max:.2}")]
    BudgetExceeded { current: f32, max: f32 },
    
    #[error("Network error: {0}")]
    Network(#[from] reqwest::Error),
    
    #[error("Parsing error: {0}")]
    Parsing(String),
}
```

### 9.2 Logging Standards

```rust
// Use tracing crate
use tracing::{info, warn, error, debug};

info!(url = %url, platform = ?platform, "Starting content extraction");
debug!(cache_hit = true, "Cache hit for URL");
warn!(url = %url, error = %e, "Extraction failed, skipping");
error!(budget = %budget.current_cost, "Budget exceeded, stopping");
```

### 9.3 Privacy Guidelines

```yaml
Data Retention:
  - Cache: 7 days TTL
  - Transcripts: Store locally only
  - URLs: Strip query parameters with tokens
  - PII: Detect and anonymize in transcripts

User Consent:
  - First run: Ask for Whisper API permission
  - Display cost estimates before transcription
  - Allow disabling content extraction per platform
```

---

## üîü CHECKLIST DE EJECUCI√ìN

### Pre-Implementation Checklist

- [ ] Review this document with team
- [ ] Decide on Phase 1 scope (Infrastructure)
- [ ] Estimate effort for each phase
- [ ] Setup development environment:
  - [ ] Install yt-dlp
  - [ ] Install ffmpeg
  - [ ] Get Spotify API credentials
  - [ ] Get OpenAI API key
- [ ] Create feature branch: `feat/content-extraction`

### Phase 1: Infrastructure

- [ ] Create `src/data_import/content_extractors/` directory
- [ ] Implement `mod.rs` with common traits
- [ ] Implement `cache.rs` (SQLite)
- [ ] Implement `rate_limiter.rs`
- [ ] Implement `budget.rs`
- [ ] Implement `dependency_checker.rs`
- [ ] Write tests for cache
- [ ] Write tests for rate limiter
- [ ] Update `Cargo.toml` dependencies
- [ ] Document API in rustdoc

### Phase 2: YouTube Extractor

- [ ] Implement `youtube.rs`
- [ ] Test yt-dlp CLI integration
- [ ] Parse JSON metadata
- [ ] Extract VTT subtitles
- [ ] Handle errors gracefully
- [ ] Write integration tests (real videos)
- [ ] Update `18_hyperlink-extractor.md` with YouTube details
- [ ] Benchmark performance (<5s target)

### Phase 3: Webpage Scraper

- [ ] Implement `webpage.rs`
- [ ] Integrate `scraper` crate
- [ ] Extract Open Graph tags
- [ ] Extract main content (Readability)
- [ ] Detect language
- [ ] Write integration tests (real websites)
- [ ] Update documentation
- [ ] Benchmark performance (<2s target)

### Phase 4: Audio Transcription

- [ ] Implement `audio.rs`
- [ ] Integrate Whisper API
- [ ] Implement budget tracking
- [ ] Add local Whisper fallback
- [ ] Write integration tests
- [ ] Cost analysis documentation
- [ ] Update CHECKLIST_V2.md

### Phase 5: Spotify Integration

- [ ] Implement `spotify.rs`
- [ ] Setup OAuth flow
- [ ] Fetch track metadata
- [ ] Fetch playlist metadata
- [ ] Write integration tests
- [ ] Update documentation

### Phase 6: Video Processing

- [ ] Implement `video.rs`
- [ ] Integrate ffmpeg/ffprobe
- [ ] Extract audio track
- [ ] Transcribe audio
- [ ] Write integration tests
- [ ] Benchmark performance

### Post-Implementation

- [ ] Update `18_hyperlink-extractor.md` (parent doc)
- [ ] Update CHECKLIST_V2.md with new tasks
- [ ] Write user guide for content extraction
- [ ] Setup monitoring/alerts
- [ ] Code review + merge to main
- [ ] Release v1.1.0 üéâ

---

## üìö REFERENCIAS

### External Tools

1. **yt-dlp**
   - Repo: https://github.com/yt-dlp/yt-dlp
   - Docs: https://github.com/yt-dlp/yt-dlp#usage-and-options
   - Install: `pip install yt-dlp`

2. **ffmpeg**
   - Website: https://ffmpeg.org/
   - Docs: https://ffmpeg.org/documentation.html
   - Install: `sudo apt install ffmpeg`

3. **Spotify Web API**
   - Docs: https://developer.spotify.com/documentation/web-api
   - OAuth Guide: https://developer.spotify.com/documentation/web-api/tutorials/getting-started

4. **OpenAI Whisper API**
   - Docs: https://platform.openai.com/docs/guides/speech-to-text
   - Pricing: $0.006/minute
   - Models: whisper-1

### Rust Crates

```toml
[dependencies]
# Existing
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }

# New for content extraction
scraper = "0.17"           # HTML parsing
sqlx = { version = "0.7", features = ["sqlite", "runtime-tokio"] }  # Cache
rspotify = "0.12"          # Spotify API
lingua-rs = "1.4"          # Language detection
thiserror = "1.0"          # Error handling
tracing = "0.1"            # Logging

# Optional
whisper-rs = "0.9"         # Local Whisper fallback
```

### Related Documents

1. **Parent Document:**
   - `ROADMAP_V2/02_COMPONENTES/18_hyperlink-extractor.md`
   - Section "Future Enhancements - Phase 6: Metadata Fetching"

2. **Sensory Engine:**
   - `ROADMAP_V2/02_COMPONENTES/01_sensory-engine.md`
   - Whisper API integration reference

3. **LLM Client:**
   - `src/multi_agent/llm_client.rs`
   - HTTP client pattern with reqwest

4. **CHECKLIST:**
   - `ROADMAP_V2/CHECKLIST_V2.md`
   - Task 7.x.3.8: HyperlinkExtractor

---

**Estado:** üéØ READY FOR REVIEW  
**Next Steps:** Review ‚Üí Phase 1 Implementation ‚Üí Testing  
**Owner:** Eduardo GJ  
**Reviewed By:** [Pending]
